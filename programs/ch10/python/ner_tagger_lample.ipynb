{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "Implementation of a LSTM NER tagger following the paper _Neural Architectures for Named Entity Recognition_ from 2016 by Lample, Ballesteros, Subramanian, Kawakami, and Dyer.\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guillaume Lample used Theano and Python 2, and his code is available here: https://github.com/glample/tagger. Lample et .al reported a F1 performance of 89.15 without conditional random fields (CRF) and 90.94 with CRF.\n",
    "\n",
    "Here, I used Keras and Python 3, and I set aside the CRF layer as the official Keras does not support any implementation of it. The available CRF layers I tested were quite brittle and may not survive future Keras and Tensorflow evolutions. This implementation has a few other differences with the original one: For instance, Lample et .al used skip-n-grams, while I used Glove.\n",
    "\n",
    "I set the hyperparameters following two papers by Reimers and Gurevych (2017):\n",
    "* _Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging_ in Proc. of EMNLP, where they obtained a median performance of 90.81, slightly lower than the one reported by Lample's and al. (http://aclweb.org/anthology/D17-1035);\n",
    "* _Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks_, where they quantify the influence of parameters from a collection of 50,000 runs. Paper: https://arxiv.org/abs/1707.06799 and code: https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf.\n",
    "\n",
    "I took the parameters that had most influence and that only required an option change in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import numpy as np\n",
    "import statistics\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import base\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, TimeDistributed, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "# from tf2crf import CRF, ModelWithCRFLoss\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "CHAR_LSTM_UNITS = 25\n",
    "LSTM_UNITS = 100\n",
    "EMBEDDING_DIM = 100\n",
    "CHAR_EMBEDDING_DIM = 25\n",
    "RAND_INIT_RANGE = 0.05\n",
    "PROB_SUBST = 0.5\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 15\n",
    "BINS = True\n",
    "LOWER = True    # Tells if the words will be lowercased or not. \n",
    "                # This should match the embeddings case. We do not lowercase the character input\n",
    "\n",
    "# Defaults\n",
    "MAX_SENT_LEN_TRAIN = 150\n",
    "MAX_SENT_LEN_DEV = 150\n",
    "MAX_SENT_LEN_TEST = 150\n",
    "MAX_WORD_LEN_TRAIN = 30\n",
    "MAX_WORD_LEN_DEV = 30\n",
    "MAX_WORD_LEN_TEST = 30\n",
    "\n",
    "COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We use the CoNLL dataset on named entity recognition. There were two conferences on it: <a href=\"https://www.clips.uantwerpen.be/conll2002/ner/\">CoNLL 2002</a> (Spanish and Dutch) and <a href=\"https://www.clips.uantwerpen.be/conll2003/ner/\">CoNLL 2003</a> (English and German). We work here work on the English dataset.\n",
    "3. The dataset comes in the form of three files: a training set, a development set, and a test set. <!--, named:\n",
    "    <tt>eng.train</tt>, <tt>eng.testa</tt> (validation), and <tt>eng.testb</tt> (test).-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    EMBEDDING_DIR = '/content/drive/My Drive/Colab Notebooks/corpus/'\n",
    "    EMBEDDING_FILE = 'glove.6B.100d.txt'\n",
    "    BASE_DIR = '/content/drive/My Drive/Colab Notebooks/corpus/'\n",
    "    RESULTS_DIR = '/content/drive/My Drive/Colab Notebooks/'\n",
    "    MODELS_DIR = '/content/drive/My Drive/Colab Notebooks/'\n",
    "else:\n",
    "    EMBEDDING_DIR = '/Users/pierre/Documents/Cours/EDAN20/corpus/'\n",
    "    EMBEDDING_FILE = 'glove.6B.100d.txt'\n",
    "    BASE_DIR = '/Users/pierre/Projets/Corpora/CoNLL2003/'\n",
    "    RESULTS_DIR = '/Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/results/'\n",
    "    MODELS_DIR = '/Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the corpus with the cells below. The functions will enable us to load the files in the form of a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll2003_en():\n",
    "    #train_file = BASE_DIR + 'NER-data/eng.train'\n",
    "    #dev_file = BASE_DIR + 'NER-data/eng.valid'\n",
    "    #test_file = BASE_DIR + 'NER-data/eng.test'\n",
    "    train_file = BASE_DIR + 'ner-bio/train.txt'\n",
    "    dev_file = BASE_DIR + 'ner-bio/valid.txt'\n",
    "    test_file = BASE_DIR + 'ner-bio/test.txt'    \n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file, encoding='utf8').read().strip()\n",
    "    dev_sentences = open(dev_file, encoding='utf8').read().strip()\n",
    "    test_sentences = open(test_file, encoding='utf8').read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-DOCSTART- -X- -X- O\\n\\nEU NNP B-NP B-ORG\\nrejects VBZ B-VP O\\nGerman JJ B-NP B-MISC\\ncall NN I-NP O\\nto T'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer(base.TransformerMixin):\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}\n",
      "{'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}\n",
      "{'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}\n",
      "{'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}\n",
      "{'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}\n",
      "{'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}\n",
      "{'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}\n",
      "{'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}\n",
      "{'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[0])\n",
    "print('\\n'.join(map(str, train_dict[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the models with the training set and use the test set to evaluate them. For this, we will apply the `conlleval` script that will compute the harmonic mean of the precision and recall: F1. \n",
    "\n",
    "`conlleval` was written in Perl. Some people rewrote it in Python and we use the translation here. The line below installs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conlleval in /Users/pierre/opt/anaconda3/lib/python3.8/site-packages (0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install conlleval\n",
    "import conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent the words with the GloVe embeddings:\n",
    "1. We download the GloVe embeddings 6B from <a href=\"https://nlp.stanford.edu/projects/glove/\">https://nlp.stanford.edu/projects/glove/</a> and keep the 100d vectors.\n",
    "2. We apply a function that reads GloVe embeddings and store them in a dictionary, where the keys will be the words and the values, the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in embedding dictionary: 400000\n"
     ]
    }
   ],
   "source": [
    "# We read the embeddings\n",
    "embedding_file = EMBEDDING_DIR + EMBEDDING_FILE\n",
    "embeddings_dict = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "print('# words in embedding dictionary:', len(embedded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(embeddings_dict[word]) for word in embeddings_dict.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the $\\mathbf{X}$ and $\\mathbf{Y}$ Lists of Symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True)` function that for each sentence returns the $\\mathbf{x}$ and $\\mathbf{y}$ lists of symbols consisting of words and NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(corpus_dict, key_x='form', key_y='pos', tolower=True):\n",
    "    \"\"\"\n",
    "    Creates sequences from a list of dictionaries\n",
    "    :param corpus_dict:\n",
    "    :param key_x:\n",
    "    :param key_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = [word[key_x] for word in sentence]\n",
    "        y = [word[key_y] for word in sentence]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_cat, Y_train_cat = build_sequences(train_dict, key_x='form', key_y='ner', tolower=False)\n",
    "X_dev_word_cat, Y_dev_cat = build_sequences(dev_dict, key_x='form', key_y='ner', tolower=False)\n",
    "X_test_word_cat, Y_test_cat = build_sequences(test_dict, key_x='form', key_y='ner', tolower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence, word input ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "First sentence, NER output ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('First sentence, word input', X_train_word_cat[1])\n",
    "print('First sentence, NER output', Y_train_cat[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the $\\mathbf{X}$ Symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary from CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collect the words in their original case from the three sets\n",
    "vocabulary_words_train = tuple([word\n",
    "                          for sentence in X_train_word_cat\n",
    "                          for word in sentence])\n",
    "vocabulary_words_dev = tuple([word\n",
    "                        for sentence in X_dev_word_cat\n",
    "                        for word in sentence])\n",
    "vocabulary_words_test = tuple([word\n",
    "                         for sentence in X_test_word_cat\n",
    "                         for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words: 23624\n"
     ]
    }
   ],
   "source": [
    "# We extract the unique words in their original case\n",
    "#vocabulary_words_conll = list(vocabulary_words_train)\n",
    "vocabulary_words_conll = tuple(sorted(list(set(vocabulary_words_train))))\n",
    "print('#words:', len(vocabulary_words_conll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singletons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lample et al. use the singletons to train the embeddings of the unknown word symbol. We extract them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10060"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We extract the singletons\n",
    "if LOWER:\n",
    "    counts = Counter(map(str.lower, list(vocabulary_words_train)))\n",
    "else:\n",
    "    counts= Counter(vocabulary_words_train)\n",
    "singletons = [k for k, v in counts.items() if counts[k] == 1]\n",
    "len(singletons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters from CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collect all the unique characters in their original case\n",
    "chars = set(''.join(vocabulary_words_conll))\n",
    "chars = tuple(sorted(list(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#chars: 84\n"
     ]
    }
   ],
   "source": [
    "NB_CHARS = len(chars)\n",
    "print('#chars:', NB_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats on the the word lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal length of a word, mean, and reasonable upperbound of a word length. Not used in the rest of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word maximal length 61\n"
     ]
    }
   ],
   "source": [
    "MAX_WORD_LEN = max([len(word) for word in vocabulary_words_conll])\n",
    "print('Word maximal length', MAX_WORD_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.832839485269218"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean([len(word) for word in vocabulary_words_conll])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.080759869694724"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean([len(word) for word in vocabulary_words_conll]) + 3 * statistics.stdev(\n",
    "    [len(word) for word in vocabulary_words_conll])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary from CoNLL and the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a vocabulary of all the words observed in the training set and the words in the pretrained embeddings. We merge the list of unique CoNLL words with the words in the embedding file and we sort them. We store the vocabulary in `vocabulary_words`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We align the case the CoNLL words with the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique (possibly lowercased) words in the CoNLL vocabulary: 21010\n"
     ]
    }
   ],
   "source": [
    "if LOWER:\n",
    "    vocabulary_words = set(map(str.lower, vocabulary_words_conll))\n",
    "else:\n",
    "    vocabulary_words = set(vocabulary_words_conll)\n",
    "print('# unique (possibly lowercased) words in the CoNLL vocabulary:',\n",
    "      len(vocabulary_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding vocabulary of embedded words\n",
    "vocabulary_words = tuple(sorted(list(set(embedded_words + list(vocabulary_words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in the vocabulary: embeddings and CoNLL corpus: 402595\n"
     ]
    }
   ],
   "source": [
    "print('# words in the vocabulary: embeddings and CoNLL corpus:', len(vocabulary_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the $\\mathbf{Y}$ Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 named entity tags: ('B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O')\n"
     ]
    }
   ],
   "source": [
    "ner = tuple(sorted(set([ner\n",
    "                  for sentence in Y_train_cat\n",
    "                  for ner in sentence])))\n",
    "NB_CLASSES = len(ner)\n",
    "print(NB_CLASSES, 'named entity tags:', ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters, Words, and NER Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the indices `word2idx`, `ner2idx` and inverted indices `idx2word`, `idx2ner` for the words and the NER. We use index 0 for the padding symbol and 1 for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the indexes\n",
    "# For the words and chars, we start at two to make provision for\n",
    "# the padding symbol 0 in RNN and LSTMs and unknown words/chars, 1\n",
    "# We do not need it for the outpout\n",
    "idx2char = dict(enumerate(chars, start=2))\n",
    "idx2word = dict(enumerate(vocabulary_words, start=2))\n",
    "idx2ner = dict(enumerate(ner, start=0))\n",
    "char2idx = {v: k for k, v in idx2char.items()}\n",
    "word2idx = {v: k for k, v in idx2word.items()}\n",
    "ner2idx = {v: k for k, v in idx2ner.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char index: [('!', 2), ('\"', 3), ('$', 4), ('%', 5), ('&', 6), (\"'\", 7), ('(', 8), (')', 9), ('*', 10), ('+', 11)]\n",
      "word index: [('!', 2), ('!!', 3), ('!!!', 4), ('!!!!', 5), ('!!!!!', 6), ('!?', 7), ('!?!', 8), ('\"', 9), ('#', 10), ('##', 11)]\n",
      "NER index: [('B-LOC', 0), ('B-MISC', 1), ('B-ORG', 2), ('B-PER', 3), ('I-LOC', 4), ('I-MISC', 5), ('I-ORG', 6), ('I-PER', 7), ('O', 8)]\n"
     ]
    }
   ],
   "source": [
    "print('char index:', list(char2idx.items())[:10])\n",
    "print('word index:', list(word2idx.items())[:10])\n",
    "print('NER index:', list(ner2idx.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the $\\mathbf{X}$ and $\\mathbf{Y}$ Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the input and output sequences with numerical indices. First, we convert the $\\mathbf{X}$ and $\\mathbf{Y}$ lists of symbols in lists of numbers using the indices we created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chararacters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_char_index(X, idx, UKN_IDX=1):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or POS lists) to char indexes\n",
    "    :param X: List of word (or POS) lists\n",
    "    :param idx: char to number dictionary\n",
    "    :return: A list of x, where x is a list of char lists\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for xl in X:\n",
    "        # We map the unknown symbols to one\n",
    "        x_idx = [list(map(lambda x: idx.get(x, UKN_IDX), list(x))) for x in xl]\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the parallel sequences of indexes for the chars in their original case\n",
    "X_train_char_idx = to_char_index(X_train_word_cat, char2idx)\n",
    "X_dev_char_idx = to_char_index(X_dev_word_cat, char2idx)\n",
    "X_test_char_idx = to_char_index(X_test_word_cat, char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-DOCSTART-'],\n",
       " ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_word_cat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[13, 34, 45, 33, 49, 50, 31, 48, 50, 13]],\n",
       " [[35, 51],\n",
       "  [77, 64, 69, 64, 62, 79, 78],\n",
       "  [37, 64, 77, 72, 60, 73],\n",
       "  [62, 60, 71, 71],\n",
       "  [79, 74],\n",
       "  [61, 74, 84, 62, 74, 79, 79],\n",
       "  [32, 77, 68, 79, 68, 78, 67],\n",
       "  [71, 60, 72, 61],\n",
       "  [14]]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_char_idx[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the words in lower case.\n",
    "if LOWER:\n",
    "    X_train_word_cat = [[x.lower() for x in x_vect] for x_vect in X_train_word_cat]\n",
    "    X_dev_word_cat = [[x.lower() for x in x_vect] for x_vect in X_dev_word_cat]\n",
    "    X_test_word_cat = [[x.lower() for x in x_vect] for x_vect in X_test_word_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_word_cat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx, UKN_IDX=1):\n",
    "    \"\"\"\n",
    "    Convert the word lists (or POS lists) to indexes\n",
    "    :param X: List of word (or POS) lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown symbols to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, UKN_IDX), x))\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the parallel sequences of indexes for the words\n",
    "# Train\n",
    "X_train_word_idx = to_index(X_train_word_cat, word2idx)\n",
    "# Dev\n",
    "X_dev_word_idx = to_index(X_dev_word_cat, word2idx)\n",
    "# Test\n",
    "X_test_word_idx = to_index(X_test_word_cat, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train. Word indices: [[935], [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936]]\n",
      "Dev. Word indices: [[935], [113351, 679, 221875, 354360, 275584, 63471, 364505, 49150, 192163, 381011, 936]]\n",
      "Test. Word indices: [[935], [338644, 679, 197600, 162137, 229067, 390518, 517, 100680, 190291, 350949, 120818, 936]]\n"
     ]
    }
   ],
   "source": [
    "print('Train. Word indices:', X_train_word_idx[:2])\n",
    "print('Dev. Word indices:', X_dev_word_idx[:2])\n",
    "print('Test. Word indices:', X_test_word_idx[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NER Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "Y_train_idx = to_index(Y_train_cat, ner2idx)\n",
    "# Dev\n",
    "Y_dev_idx = to_index(Y_dev_cat, ner2idx)\n",
    "# Test\n",
    "Y_test_idx = to_index(Y_test_cat, ner2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train. NER indices: [[8], [2, 8, 1, 8, 8, 8, 1, 8, 8]]\n",
      "Dev. NER indices: [[8], [8, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8]]\n",
      "Test. NER indices: [[8], [8, 8, 0, 8, 8, 8, 8, 3, 8, 8, 8, 8]]\n"
     ]
    }
   ],
   "source": [
    "print('Train. NER indices:', Y_train_idx[:2])\n",
    "print('Dev. NER indices:', Y_dev_idx[:2])\n",
    "print('Test. NER indices:', Y_test_idx[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pad the characters of a word to have a a length of `MAX_WORD_LEN`. We compute it for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN_TRAIN = max([len(word)  for sentence in X_train_char_idx for word in sentence])\n",
    "MAX_WORD_LEN_DEV = max([len(word)  for sentence in X_dev_char_idx for word in sentence])\n",
    "MAX_WORD_LEN_TEST = max([len(word)  for sentence in X_test_char_idx for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_char_idx = list(map(lambda x: pad_sequences(x, maxlen=MAX_WORD_LEN_TRAIN, padding='post'),\n",
    "                            X_train_char_idx))\n",
    "X_dev_char_idx = list(map(lambda x: pad_sequences(x, maxlen=MAX_WORD_LEN_DEV, padding='post'),\n",
    "                          X_dev_char_idx))\n",
    "X_test_char_idx = list(map(lambda x: pad_sequences(x, maxlen=MAX_WORD_LEN_TEST, padding='post'),\n",
    "                           X_test_char_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[13, 34, 45, 33, 49, 50, 31, 48, 50, 13,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32),\n",
       " array([[35, 51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [77, 64, 69, 64, 62, 79, 78,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [37, 64, 77, 72, 60, 73,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [62, 60, 71, 71,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [79, 74,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [61, 74, 84, 62, 74, 79, 79,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [32, 77, 68, 79, 68, 78, 67,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [71, 60, 72, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_char_idx[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pad the lists of lists of characters to have a length of `MAX_SENT_LEN_TRAIN`. We determine it for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LEN_TRAIN = max(map(len, X_train_word_idx))\n",
    "MAX_SENT_LEN_DEV = max(map(len, X_dev_word_idx))\n",
    "MAX_SENT_LEN_TEST = max(map(len, X_test_word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_char_idx = pad_sequences(X_train_char_idx, maxlen=MAX_SENT_LEN_TRAIN, padding='post')\n",
    "X_dev_char_idx = pad_sequences(X_dev_char_idx, maxlen=MAX_SENT_LEN_DEV, padding='post')\n",
    "X_test_char_idx = pad_sequences(X_test_char_idx, maxlen=MAX_SENT_LEN_TEST, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[13, 34, 45, 33, 49, 50, 31, 48, 50, 13,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32),\n",
       " array([[35, 51,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [77, 64, 69, 64, 62, 79, 78,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [37, 64, 77, 72, 60, 73,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [62, 60, 71, 71,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [79, 74,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [61, 74, 84, 62, 74, 79, 79,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [32, 77, 68, 79, 68, 78, 67,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [71, 60, 72, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_char_idx[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad the word lists. Here we only pad the dev and test set to be able to predict in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_word_idx = pad_sequences(X_train_word_idx, maxlen=MAX_SENT_LEN_TRAIN, padding='post')\n",
    "X_dev_word_idx = pad_sequences(X_dev_word_idx, maxlen=MAX_SENT_LEN_DEV, padding='post')\n",
    "X_test_word_idx = pad_sequences(X_test_word_idx, maxlen=MAX_SENT_LEN_TEST, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[935], [142143, 307143, 161836, 91321, 363368, 83766, 85852, 218260, 936]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_word_idx[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad the NER lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train_idx = pad_sequences(Y_train_idx, maxlen=MAX_SENT_LEN_TRAIN, padding='post')\n",
    "Y_dev_idx = pad_sequences(Y_dev_idx, maxlen=MAX_SENT_LEN_DEV, padding='post')\n",
    "Y_test_idx = pad_sequences(Y_test_idx, maxlen=MAX_SENT_LEN_TEST, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8], [2, 8, 1, 8, 8, 8, 1, 8, 8]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_idx[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the training procedure, we need to pad or not the training set:\n",
    "1. If we use `fit`, we have to pad;\n",
    "2. We can also create bins of same length and train with `train_on_batch`. This allows for a more flexible training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BINS:\n",
    "    X_train_char_idx = pad_sequences(X_train_char_idx, maxlen=MAX_SENT_LEN_TRAIN, padding='post')\n",
    "    X_train_word_idx = pad_sequences(X_train_word_idx, maxlen=MAX_SENT_LEN_TRAIN, padding='post')\n",
    "    Y_train_idx = pad_sequences(Y_train_idx, maxlen=MAX_SENT_LEN_TRAIN, padding='post')  \n",
    "training_bins = [list(zip(X_train_word_idx, X_train_char_idx, Y_train_idx))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating bins of same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create bins of sentences of the same length. This avoids padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINS:\n",
    "    training_set_sorted = sorted(zip(X_train_word_idx, \n",
    "                           X_train_char_idx, \n",
    "                           Y_train_idx), \n",
    "                       key=lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the histogram of sentence lengths. `groupby` is like the `uniq -c` shell command. We obtain a list of pairs: (sentence length, count of sentences that have this length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "if BINS:\n",
    "    train_sent_len_list = list(map(lambda x: len(x[0]), training_set_sorted))\n",
    "    train_length_hist = [(x[0], len(list(x[1]))) for x in groupby(train_sent_len_list)]\n",
    "    train_length_hist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if BINS:\n",
    "    plt.bar(list(map(lambda x: x[0], train_length_hist)),\n",
    "        list(map(lambda x: x[1], train_length_hist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINS:\n",
    "    training_bins = []\n",
    "    for bin in train_length_hist:\n",
    "        training_bins += [training_set_sorted[:bin[1]]]\n",
    "        training_set_sorted = training_set_sorted[bin[1]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print a few sentences of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([284434, 79019], array([[46, 64, 79, 64, 77,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [32, 71, 60, 62, 70, 61, 80, 77, 73,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32), [3, 7])\n",
      "([86920, 15423], array([[32, 48, 51, 49, 49, 35, 42, 49,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [17, 25, 25, 22, 13, 16, 24, 13, 18, 18,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32), [0, 8])\n",
      "([227217, 15423], array([[42, 45, 44, 34, 45, 44,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [17, 25, 25, 22, 13, 16, 24, 13, 18, 18,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32), [0, 8])\n",
      "([73591, 15423], array([[32, 35, 39, 40, 39, 44, 37,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [17, 25, 25, 22, 13, 16, 24, 13, 18, 18,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32), [0, 8])\n",
      "([73591, 15423], array([[32, 35, 39, 40, 39, 44, 37,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [17, 25, 25, 22, 13, 16, 24, 13, 18, 18,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32), [0, 8])\n"
     ]
    }
   ],
   "source": [
    "if BINS:\n",
    "    for i in range(5):\n",
    "        print(training_bins[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix of dimensions $(M, N)$, where $M$ will be the size of the vocabulary: The unique words in the training set and the words in GloVe, and $N$, the dimension of the embeddings.\n",
    "The padding symbol and the unknown word symbol will be part of the vocabulary. The shape of your matrix should be: (402597, 100). Initialize it with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# We add two dimensions for the padding symbol at index 0 and unknown words at index 1\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "# embedding_matrix = np.random.random((len(vocabulary_words) + 2, EMBEDDING_DIM))\n",
    "# embedding_matrix = np.zeros((len(vocabulary_words) + 2, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402597, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the matrix with the GloVe embeddings when available. You will use the indices from the previous section. You will call `out_of_embeddings` the list of words in CoNLL, but not in the embedding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_embeddings = []\n",
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word2idx[word]] = embeddings_dict[word]\n",
    "    else:\n",
    "        # Otherwise, it keeps a random value in the matrix\n",
    "        # We store the out of vocabulary words\n",
    "        out_of_embeddings += [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of embeddings: 2595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['zelezarny',\n",
       " 'zhilan',\n",
       " 'zieger',\n",
       " 'zighayer',\n",
       " 'zilinskiene',\n",
       " 'zirka-nibas',\n",
       " 'zuleeg',\n",
       " 'zundra',\n",
       " 'zwingmann',\n",
       " 'zyrecha']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Out of embeddings:', len(out_of_embeddings))\n",
    "out_of_embeddings[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of the padding symbol, idx 0, random numbers [ 0.00488135  0.02151894  0.01027634  0.00448832 -0.00763452  0.01458941\n",
      " -0.00624128  0.0391773   0.04636628 -0.01165585]\n",
      "Embedding of table [-0.61453998  0.89692998  0.56770998  0.39102    -0.22437     0.49035001\n",
      "  0.10868     0.27410999 -0.23833001 -0.52152997]\n",
      "Embedding of zwingmann, random numbers [ 0.00782644 -0.02621394  0.04756559  0.00408949  0.01875676  0.01106755\n",
      "  0.01659441  0.02207432 -0.03631456  0.00730187]\n"
     ]
    }
   ],
   "source": [
    "print('Embedding of the padding symbol, idx 0, random numbers', embedding_matrix[0][:10])\n",
    "print('Embedding of table', embedding_matrix[word2idx['table']][:10])\n",
    "if LOWER:\n",
    "    print('Embedding of zwingmann, random numbers', embedding_matrix[word2idx['zwingmann']][:10])\n",
    "else:\n",
    "    print('Embedding of Zwingmann, random numbers', embedding_matrix[word2idx['Zwingmann']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char input\n",
    "# shape: (MAX_SENT_LEN, MAX_WORD_LEN,)\n",
    "char_input = Input(shape=(None, None,))\n",
    "embedded_chars = Embedding(input_dim=NB_CHARS + 2,\n",
    "                                 output_dim=CHAR_EMBEDDING_DIM,\n",
    "                                 trainable=True,\n",
    "                                 mask_zero=True,\n",
    "                                 embeddings_initializer=RandomUniform(minval=-RAND_INIT_RANGE,\n",
    "                                                                      maxval=RAND_INIT_RANGE))(char_input)\n",
    "encoded_chars = TimeDistributed(Bidirectional(LSTM(CHAR_LSTM_UNITS, return_sequences=False)))(embedded_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word input\n",
    "# shape: (MAX_SENT_LEN,)\n",
    "word_input = Input(shape=(None,))\n",
    "embedded_words = Embedding(len(vocabulary_words) + 2,\n",
    "                           EMBEDDING_DIM,\n",
    "                           trainable=True,\n",
    "                           mask_zero=True,\n",
    "                           weights=[embedding_matrix])(word_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenation\n",
    "final_embeddings = concatenate([encoded_chars, embedded_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout\n",
    "final_embeddings = Dropout(0.5)(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_output = Bidirectional(LSTM(LSTM_UNITS,\n",
    "                                 recurrent_dropout=0.5,\n",
    "                                 return_sequences=True))(final_embeddings)\n",
    "\"\"\"lstm_output = Dropout(0.5)(lstm_output)\n",
    "lstm_output = Bidirectional(LSTM(LSTM_UNITS,\n",
    "                                 recurrent_dropout=0.5,\n",
    "                                 return_sequences=True))(lstm_output)\"\"\"\n",
    "out = Dense(len(ner), activation='softmax')(lstm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([char_input, word_input], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, None, 2 2150        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 50)     10200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    40259700    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 150)    0           time_distributed[0][0]           \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 150)    0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 200)    200800      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9)      1809        bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 40,474,659\n",
      "Trainable params: 40,474,659\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Nadam(clipnorm=1.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of an annotated corpus: We add `pner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_column_names = ['form', 'ppos', 'pchunk', 'ner', 'pner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2conll(corpus_dict, column_names):\n",
    "    \"\"\"\n",
    "    Saves the corpus in a file\n",
    "    :param file:\n",
    "    :param corpus_dict:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corpus_str = ''\n",
    "    for sentence in corpus_dict:\n",
    "        sentence_lst = []\n",
    "        for row in sentence:\n",
    "            items = map(lambda x: row.get(x, '_'), column_names)\n",
    "            sentence_lst += ' '.join(items) + '\\n'\n",
    "        sentence_lst += '\\n'\n",
    "        sentence_str = ''.join(sentence_lst)\n",
    "        corpus_str += sentence_str\n",
    "    return corpus_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_generator(dataset):\n",
    "    # We take the sentences of the same length\n",
    "    random.shuffle(dataset)\n",
    "    for sentence_bin in dataset:\n",
    "        random.shuffle(sentence_bin)\n",
    "        # The number of batches. \n",
    "        batch_cnt = int(np.ceil((len(sentence_bin) / BATCH_SIZE)))\n",
    "        # We create batches of BATCH_SIZE\n",
    "        for j in range(batch_cnt):\n",
    "            char_batch = []\n",
    "            token_batch = []\n",
    "            label_batch = []\n",
    "            for sentence in sentence_bin[j*BATCH_SIZE:(j+1)*BATCH_SIZE]:\n",
    "                tokens, chars, labels = sentence\n",
    "                char_batch.append(chars)\n",
    "                token_batch.append(tokens)\n",
    "                label_batch.append(labels)\n",
    "            #label_batch = list(map(lambda x: np.transpose([x]), label_batch))\n",
    "            #label_batch = to_categorical(label_batch, num_classes=len(ner))\n",
    "            yield  np.asarray(char_batch), np.asarray(token_batch), np.asarray(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "CoNLL score dev set\n",
      "dev 0.8038553422851963\n",
      "CoNLL score test set\n",
      "test 0.7711464147702877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  1%|          | 1/100 [03:00<4:58:17, 180.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "CoNLL score dev set\n",
      "dev 0.8543385021445068\n",
      "CoNLL score test set\n",
      "test 0.8381282495667244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  2%|         | 2/100 [05:47<4:41:48, 172.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "CoNLL score dev set\n",
      "dev 0.8761857214178732\n",
      "CoNLL score test set\n",
      "test 0.8514540214828399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  3%|         | 3/100 [08:34<4:34:57, 170.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "CoNLL score dev set\n",
      "dev 0.8821286472148542\n",
      "CoNLL score test set\n",
      "test 0.8573660908064936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  4%|         | 4/100 [11:23<4:31:29, 169.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "CoNLL score dev set\n",
      "dev 0.8940253671562083\n",
      "CoNLL score test set\n",
      "test 0.8711484593837536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  5%|         | 5/100 [14:07<4:25:28, 167.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "CoNLL score dev set\n",
      "dev 0.9025162472921181\n",
      "CoNLL score test set\n",
      "test 0.8702010968921389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  6%|         | 6/100 [16:49<4:19:15, 165.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "CoNLL score dev set\n",
      "dev 0.907185628742515\n",
      "CoNLL score test set\n",
      "test 0.8708551483420593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  7%|         | 7/100 [19:19<4:09:04, 160.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "CoNLL score dev set\n",
      "dev 0.9144434222631095\n",
      "CoNLL score test set\n",
      "test 0.8759839076438692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  8%|         | 8/100 [21:59<4:05:54, 160.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "CoNLL score dev set\n",
      "dev 0.9148438804474871\n",
      "CoNLL score test set\n",
      "test 0.8821212916776057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      "  9%|         | 9/100 [24:44<4:05:22, 161.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "CoNLL score dev set\n",
      "dev 0.9183724621940013\n",
      "CoNLL score test set\n",
      "test 0.8800070120080639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 10%|         | 10/100 [27:24<4:01:42, 161.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "CoNLL score dev set\n",
      "dev 0.9194574681848627\n",
      "CoNLL score test set\n",
      "test 0.8800561551285426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 11%|         | 11/100 [30:08<4:00:25, 162.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "CoNLL score dev set\n",
      "dev 0.9221757322175732\n",
      "CoNLL score test set\n",
      "test 0.8834151472650771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 12%|        | 12/100 [32:52<3:58:25, 162.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "CoNLL score dev set\n",
      "dev 0.9215571368773545\n",
      "CoNLL score test set\n",
      "test 0.8826315789473684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 13/100 [35:21<3:49:55, 158.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "CoNLL score dev set\n",
      "dev 0.9173408958211694\n",
      "CoNLL score test set\n",
      "test 0.8822859146288018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 14/100 [37:45<3:41:05, 154.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "CoNLL score dev set\n",
      "dev 0.9191851728168309\n",
      "CoNLL score test set\n",
      "test 0.8749344978165938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 15/100 [40:13<3:35:44, 152.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "CoNLL score dev set\n",
      "dev 0.9240780911062907\n",
      "CoNLL score test set\n",
      "test 0.8824914943732007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 16%|        | 16/100 [42:51<3:35:29, 153.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "CoNLL score dev set\n",
      "dev 0.9221136989732032\n",
      "CoNLL score test set\n",
      "test 0.8806087641039098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 17/100 [45:16<3:29:15, 151.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "CoNLL score dev set\n",
      "dev 0.9251507032819826\n",
      "CoNLL score test set\n",
      "test 0.8936319718928416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 18%|        | 18/100 [47:58<3:31:09, 154.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "CoNLL score dev set\n",
      "dev 0.9257620041753654\n",
      "CoNLL score test set\n",
      "test 0.8837941897094854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 19%|        | 19/100 [50:33<3:28:54, 154.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "CoNLL score dev set\n",
      "dev 0.9278471465683399\n",
      "CoNLL score test set\n",
      "test 0.8907636874945074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 20%|        | 20/100 [53:11<3:27:36, 155.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "CoNLL score dev set\n",
      "dev 0.9279211559341852\n",
      "CoNLL score test set\n",
      "test 0.8850514564800279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 21%|        | 21/100 [55:48<3:25:18, 155.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "CoNLL score dev set\n",
      "dev 0.9270450751252086\n",
      "CoNLL score test set\n",
      "test 0.8837046745303626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 22/100 [58:18<3:20:29, 154.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "CoNLL score dev set\n",
      "dev 0.9296268469822189\n",
      "CoNLL score test set\n",
      "test 0.8893166506256016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 23%|       | 23/100 [1:01:09<3:24:30, 159.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "CoNLL score dev set\n",
      "dev 0.9277420432712389\n",
      "CoNLL score test set\n",
      "test 0.8918067226890756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 24/100 [1:03:38<3:17:47, 156.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "CoNLL score dev set\n",
      "dev 0.9281111390074485\n",
      "CoNLL score test set\n",
      "test 0.8884791008078681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 25/100 [1:06:05<3:11:57, 153.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "CoNLL score dev set\n",
      "dev 0.932352203361485\n",
      "CoNLL score test set\n",
      "test 0.8896297596069486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as bidirectional_layer_call_fn, bidirectional_layer_call_and_return_conditional_losses, forward_lstm_1_layer_call_fn, forward_lstm_1_layer_call_and_return_conditional_losses, backward_lstm_1_layer_call_fn while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/pierre/Documents/Cours/EDAN20/programs/ch10/python/assets\n",
      " 26%|       | 26/100 [1:08:49<3:13:03, 156.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "CoNLL score dev set\n",
      "dev 0.9319056382800737\n",
      "CoNLL score test set\n",
      "test 0.8920080708834107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 27/100 [1:11:21<3:08:54, 155.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "CoNLL score dev set\n",
      "dev 0.9221277305319326\n",
      "CoNLL score test set\n",
      "test 0.8812817369987742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 28/100 [1:13:50<3:04:11, 153.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "CoNLL score dev set\n",
      "dev 0.9272651356993736\n",
      "CoNLL score test set\n",
      "test 0.8841442072103605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 29/100 [1:16:07<2:55:46, 148.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "CoNLL score dev set\n",
      "dev 0.9303765690376569\n",
      "CoNLL score test set\n",
      "test 0.8902171046848906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 30/100 [1:18:25<2:49:26, 145.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "CoNLL score dev set\n",
      "dev 0.9308871575772807\n",
      "CoNLL score test set\n",
      "test 0.8902567710165318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 31/100 [1:20:41<2:43:56, 142.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "CoNLL score dev set\n",
      "dev 0.9310862673118637\n",
      "CoNLL score test set\n",
      "test 0.8926573426573428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 32/100 [1:23:06<2:42:10, 143.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "CoNLL score dev set\n",
      "dev 0.9321977287909151\n",
      "CoNLL score test set\n",
      "test 0.8897052392198023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 33/100 [1:25:34<2:41:32, 144.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "CoNLL score dev set\n",
      "dev 0.9290074420938206\n",
      "CoNLL score test set\n",
      "test 0.8937664618086041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 34/100 [1:28:02<2:40:13, 145.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "CoNLL score dev set\n",
      "dev 0.9279211559341852\n",
      "CoNLL score test set\n",
      "test 0.8891811327371559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 35/100 [1:30:30<2:38:34, 146.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100\n",
      "CoNLL score dev set\n",
      "dev 0.9273425755804243\n",
      "CoNLL score test set\n",
      "test 0.8901926444833625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 36/100 [1:32:54<2:35:30, 145.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "CoNLL score dev set\n",
      "dev 0.928446188528012\n",
      "CoNLL score test set\n",
      "test 0.891846921797005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 37/100 [1:35:25<2:34:29, 147.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "CoNLL score dev set\n",
      "dev 0.9257285595337218\n",
      "CoNLL score test set\n",
      "test 0.8836884387006636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 38/100 [1:37:55<2:32:55, 147.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "CoNLL score dev set\n",
      "dev 0.9290419411323272\n",
      "CoNLL score test set\n",
      "test 0.8926820728291317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 39/100 [1:40:23<2:30:39, 148.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "CoNLL score dev set\n",
      "dev 0.9291798897611491\n",
      "CoNLL score test set\n",
      "test 0.8938223938223938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 40/100 [1:42:50<2:27:48, 147.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "CoNLL score dev set\n",
      "dev 0.9263316330796889\n",
      "CoNLL score test set\n",
      "test 0.8869595720424451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 41/100 [1:45:18<2:25:14, 147.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "CoNLL score dev set\n",
      "dev 0.9283806343906511\n",
      "CoNLL score test set\n",
      "test 0.8897851819377467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 41/100 [1:47:38<2:34:53, 157.52s/it]\n"
     ]
    }
   ],
   "source": [
    "dev_scores = []\n",
    "test_scores = []\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, EPOCHS))\n",
    "    for i, batch in enumerate(minibatch_generator(training_bins)):\n",
    "        char_batch, token_batch, label_batch = batch\n",
    "        # Symbol substitution to train the unknown word embedding\n",
    "        new_token_batch = []\n",
    "        for i, sentence in enumerate(token_batch):\n",
    "            new_token_batch += [[1 if idx_word != 0 and counts[idx2word[idx_word]] == 1 and np.random.uniform() < PROB_SUBST\n",
    "                                 else idx_word for idx_word in sentence]]\n",
    "        new_token_batch = np.array(new_token_batch)\n",
    "        loss = model.train_on_batch([char_batch, new_token_batch],\n",
    "                                    label_batch)\n",
    "\n",
    "    # We evaluate the dev set\n",
    "    print('CoNLL score dev set')\n",
    "    Y_dev_hat = model.predict([X_dev_char_idx, X_dev_word_idx])\n",
    "    for sent, y_probs_hat in zip(dev_dict, Y_dev_hat):\n",
    "        sent_len = len(sent)\n",
    "        y_probs_hat = y_probs_hat[:sent_len]\n",
    "        y_hat = map(np.argmax, y_probs_hat)\n",
    "        for word, ner_hat in zip(sent, y_hat):\n",
    "            word['pner'] = idx2ner[ner_hat]\n",
    "    conll_str = dict2conll(dev_dict, ann_column_names)\n",
    "\n",
    "    outfile = RESULTS_DIR + 'dev.out'\n",
    "    with open(outfile, 'w', encoding='utf8') as f_out:\n",
    "        f_out.write(conll_str)\n",
    "    lines = open(outfile, encoding='utf8').read().splitlines()\n",
    "    res = conlleval.evaluate(lines)\n",
    "    dev_score = res['overall']['chunks']['evals']['f1']\n",
    "    dev_scores += [dev_score] \n",
    "    print('dev', dev_score, flush=True)\n",
    "       \n",
    "    # We evaluate the test set\n",
    "    print('CoNLL score test set')\n",
    "    Y_test_hat = model.predict([X_test_char_idx, X_test_word_idx])\n",
    "    for sent, y_probs_hat in zip(test_dict, Y_test_hat):\n",
    "        sent_len = len(sent)\n",
    "        y_probs_hat = y_probs_hat[:sent_len]\n",
    "        y_hat = map(np.argmax, y_probs_hat)\n",
    "        for word, ner_hat in zip(sent, y_hat):\n",
    "            word['pner'] = idx2ner[ner_hat]\n",
    "    conll_str = dict2conll(test_dict, ann_column_names)\n",
    "\n",
    "    outfile = RESULTS_DIR + 'test.out'\n",
    "    with open(outfile, 'w', encoding='utf8') as f_out:\n",
    "        f_out.write(conll_str)\n",
    "    lines = open(outfile, encoding='utf8').read().splitlines()\n",
    "    res = conlleval.evaluate(lines)\n",
    "    test_score = res['overall']['chunks']['evals']['f1']\n",
    "    test_scores += [test_score]\n",
    "    print('test', test_score, flush=True)\n",
    "    \n",
    "    if dev_score == max(dev_scores):\n",
    "        model.save(MODELS_DIR)\n",
    "    \n",
    "    if max(dev_scores) > max(dev_scores[-PATIENCE - 1:]):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 0.932352203361485)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(dev_scores), max(dev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 0.8938223938223938)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(test_scores), max(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"callback_lists = [\\n    EarlyStopping(\\n        monitor='val_acc',\\n        patience=PATIENCE,\\n        restore_best_weights=True\\n    )\\n]\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"callback_lists = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history = model.fit([X_train_char_idx, X_train_word_idx], Y_train, epochs=EPOCHS, verbose=2,\\n                    validation_data=([X_dev_char_idx, X_dev_word_idx], Y_dev),\\n                    callbacks=callback_lists)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"history = model.fit([X_train_char_idx, X_train_word_idx], Y_train, epochs=EPOCHS, verbose=2,\n",
    "                    validation_data=([X_dev_char_idx, X_dev_word_idx], Y_dev),\n",
    "                    callbacks=callback_lists)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a figure to show the training and validation losses and accuracies and a possible overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"acc = history.history['acc']\\nval_acc = history.history['val_acc']\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\nepochs = range(1, len(acc) + 1)\\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\\nplt.title('Training and validation accuracies')\\nplt.legend()\\n\\nplt.figure()\\nplt.plot(epochs, loss, 'bo', label='Training loss')\\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\\nplt.title('Training and validation losses')\\nplt.legend()\\n\\nplt.show()\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracies')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restore the best corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the NER sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate on all the test corpus\n",
    "Y_test_hat = model.predict([X_test_char_idx, X_test_word_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict one observation at a time to avoid padding. We could have a speed up with batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y_test_hat = []\\nfor x_test_char_idx, x_test_word_idx in zip(X_test_char_idx, X_test_word_idx):\\n    y_pred_vect = model.predict([np.array([x_test_char_idx]), np.array([x_test_word_idx])])[0]\\n    Y_test_hat += [y_pred_vect]'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Y_test_hat = []\n",
    "for x_test_char_idx, x_test_word_idx in zip(X_test_char_idx, X_test_word_idx):\n",
    "    y_pred_vect = model.predict([np.array([x_test_char_idx]), np.array([x_test_word_idx])])[0]\n",
    "    Y_test_hat += [y_pred_vect]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the indices of the highest probabilities from the lists and convert them into NER values. Add them with the `pner` key to the dictionaries in the `test_dict` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'SOCCER', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': '-', 'ppos': ':', 'pchunk': 'O', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'JAPAN',\n",
       "  'ppos': 'NNP',\n",
       "  'pchunk': 'B-NP',\n",
       "  'ner': 'B-LOC',\n",
       "  'pner': 'B-LOC'},\n",
       " {'form': 'GET', 'ppos': 'VB', 'pchunk': 'B-VP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'LUCKY', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'WIN', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': ',', 'ppos': ',', 'pchunk': 'O', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'CHINA',\n",
       "  'ppos': 'NNP',\n",
       "  'pchunk': 'B-NP',\n",
       "  'ner': 'B-PER',\n",
       "  'pner': 'B-LOC'},\n",
       " {'form': 'IN', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'SURPRISE', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'DEFEAT', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O', 'pner': 'O'}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.05878094e-08, 1.10734639e-07, 2.85347340e-07, 1.22164261e-08,\n",
       "        3.98279854e-10, 1.56536771e-07, 8.59282157e-09, 5.59771918e-09,\n",
       "        9.99999404e-01],\n",
       "       [1.78205206e-09, 7.02660541e-10, 2.17378804e-09, 1.00219939e-08,\n",
       "        7.94482169e-09, 1.19151524e-08, 4.67045922e-08, 2.90979987e-08,\n",
       "        9.99999881e-01],\n",
       "       [9.99826610e-01, 7.04344120e-06, 2.58498676e-06, 4.44017996e-06,\n",
       "        5.23259644e-07, 1.51040469e-09, 3.09441228e-09, 4.42978348e-10,\n",
       "        1.58823997e-04],\n",
       "       [5.26873016e-08, 8.53360120e-07, 7.02723867e-07, 6.54960750e-05,\n",
       "        1.86408684e-07, 1.51319352e-06, 4.16333819e-07, 6.30721900e-07,\n",
       "        9.99930143e-01],\n",
       "       [1.60824580e-04, 5.01741197e-05, 1.58358878e-03, 6.15130626e-02,\n",
       "        1.96312328e-07, 9.41513235e-07, 4.84769907e-06, 1.36328545e-05,\n",
       "        9.36672747e-01]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_hat[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, y_probs_hat in zip(test_dict, Y_test_hat):\n",
    "    sent_len = len(sent)\n",
    "    y_probs_hat = y_probs_hat[:sent_len]\n",
    "    y_hat = map(np.argmax, y_probs_hat)\n",
    "    for word, ner_hat in zip(sent, y_hat):\n",
    "        word['pner'] = idx2ner[ner_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'SOCCER', 'ppos': 'NN', 'pchunk': 'B-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': '-', 'ppos': ':', 'pchunk': 'O', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'JAPAN',\n",
       "  'ppos': 'NNP',\n",
       "  'pchunk': 'B-NP',\n",
       "  'ner': 'B-LOC',\n",
       "  'pner': 'B-LOC'},\n",
       " {'form': 'GET', 'ppos': 'VB', 'pchunk': 'B-VP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'LUCKY', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'WIN', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': ',', 'ppos': ',', 'pchunk': 'O', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'CHINA',\n",
       "  'ppos': 'NNP',\n",
       "  'pchunk': 'B-NP',\n",
       "  'ner': 'B-PER',\n",
       "  'pner': 'B-LOC'},\n",
       " {'form': 'IN', 'ppos': 'IN', 'pchunk': 'B-PP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'SURPRISE', 'ppos': 'DT', 'pchunk': 'B-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': 'DEFEAT', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O', 'pner': 'O'},\n",
       " {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O', 'pner': 'O'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_str = dict2conll(test_dict, ann_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = RESULTS_DIR + 'test.out'\n",
    "with open(outfile, 'w', encoding='utf8') as f_out:\n",
    "    f_out.write(conll_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8896297596069486"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(outfile, encoding='utf8').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "score = res['overall']['chunks']['evals']['f1']\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Recognizer to Tag a Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Yesterday , I met Pierre Nugues in Dammarie'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the character input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_char_idx = to_char_index([sentence.split()], char2idx)\n",
    "MAX_WORD_LEN = max(map(len, sent_char_idx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[55, 64, 78, 79, 64, 77, 63, 60, 84],\n",
       "        [12,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [39,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [72, 64, 79,  0,  0,  0,  0,  0,  0],\n",
       "        [46, 68, 64, 77, 77, 64,  0,  0,  0],\n",
       "        [44, 80, 66, 80, 64, 78,  0,  0,  0],\n",
       "        [68, 73,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [34, 60, 72, 72, 60, 77, 68, 64,  0]]], dtype=int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_char_idx_padded = np.array(list(map(lambda x: pad_sequences(x, maxlen=MAX_WORD_LEN, padding='post'), \n",
    "                            sent_char_idx)))\n",
    "sent_char_idx_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the word input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[396267,    517, 187251, 243996, 286609,      1, 190291,      1]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_word_idx = np.array(to_index([sentence.lower().split()], word2idx))\n",
    "sent_word_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we predict the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_hat = model.predict([sent_char_idx_padded, sent_word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(idx2ner.get, list(map(np.argmax, y_probs_hat[0]))[:len(sentence.split())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on the test set using the model, where we reach a maximum on the dev set. Average on five consecutive runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One LSTM layer with the epoch number in comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.891878374774825"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = (0.8967934115997898, #59\n",
    "0.8909569798068481, #89\n",
    "0.8866736621196222, #43\n",
    "0.8965456623011339, #51\n",
    "0.8884221580467315) #51\n",
    "sum(runs)/len(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
