{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier on the *Salammb√¥* Dataset with Keras\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "We can read the data from a file with the svmlight format or directly create numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "X_train = np.array(\n",
    "    [[35680, 2217], [42514, 2761], [15162, 990], [35298, 2274],\n",
    "     [29800, 1865], [40255, 2606], [74532, 4805], [37464, 2396],\n",
    "     [31030, 1993], [24843, 1627], [36172, 2375], [39552, 2560],\n",
    "     [72545, 4597], [75352, 4871], [18031, 1119], [36961, 2503],\n",
    "     [43621, 2992], [15694, 1042], [36231, 2487], [29945, 2014],\n",
    "     [40588, 2805], [75255, 5062], [37709, 2643], [30899, 2126],\n",
    "     [25486, 1784], [37497, 2641], [40398, 2766], [74105, 5047],\n",
    "     [76725, 5312], [18317, 1215]\n",
    "     ],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Scaling and normalizing are usually very significant with neural networks. We use sklean transformers. They consist of two main methods: `fit()` and `transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9980751 , 0.06201605],\n",
       "       [0.99789774, 0.06480679],\n",
       "       [0.9978751 , 0.06515607],\n",
       "       [0.9979313 , 0.06428964],\n",
       "       [0.99804735, 0.06246169],\n",
       "       [0.9979111 , 0.06460207],\n",
       "       [0.9979283 , 0.06433539],\n",
       "       [0.99796116, 0.06382433],\n",
       "       [0.99794376, 0.06409609],\n",
       "       [0.9978623 , 0.06535129],\n",
       "       [0.9978515 , 0.06551746],\n",
       "       [0.9979119 , 0.06458977],\n",
       "       [0.99799836, 0.06324073],\n",
       "       [0.9979172 , 0.06450863],\n",
       "       [0.99807984, 0.06194062],\n",
       "       [0.9977148 , 0.06756528],\n",
       "       [0.9976559 , 0.06843004],\n",
       "       [0.99780315, 0.06624894],\n",
       "       [0.99765235, 0.06848173],\n",
       "       [0.99774593, 0.06710504],\n",
       "       [0.9976205 , 0.06894466],\n",
       "       [0.9977454 , 0.06711297],\n",
       "       [0.9975528 , 0.06991784],\n",
       "       [0.9976413 , 0.06864253],\n",
       "       [0.997559  , 0.06982835],\n",
       "       [0.99752885, 0.07025825],\n",
       "       [0.9976642 , 0.06830881],\n",
       "       [0.99768883, 0.06794866],\n",
       "       [0.99761194, 0.06906894],\n",
       "       [0.9978073 , 0.06618638]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train)\n",
    "X_train_norm = normalizer.transform(X_train)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6832309 , -1.7197777 ],\n",
       "       [ 0.5732034 , -0.56145513],\n",
       "       [ 0.431466  , -0.41648194],\n",
       "       [ 0.7831985 , -0.7761007 ],\n",
       "       [ 1.5094161 , -1.5348101 ],\n",
       "       [ 0.65675384, -0.64642584],\n",
       "       [ 0.76454884, -0.7571132 ],\n",
       "       [ 0.97006804, -0.9692323 ],\n",
       "       [ 0.861154  , -0.85643595],\n",
       "       [ 0.35127246, -0.3354572 ],\n",
       "       [ 0.28376073, -0.2664867 ],\n",
       "       [ 0.66160274, -0.6515314 ],\n",
       "       [ 1.2028158 , -1.2114625 ],\n",
       "       [ 0.6947991 , -0.685208  ],\n",
       "       [ 1.7126973 , -1.7510854 ],\n",
       "       [-0.57151246,  0.58348024],\n",
       "       [-0.9400297 ,  0.9424063 ],\n",
       "       [-0.01873669,  0.03712195],\n",
       "       [-0.96240926,  0.9638616 ],\n",
       "       [-0.37681007,  0.3924546 ],\n",
       "       [-1.1615876 ,  1.1560036 ],\n",
       "       [-0.380167  ,  0.39574805],\n",
       "       [-1.5853077 ,  1.5599338 ],\n",
       "       [-1.031413  ,  1.0306025 ],\n",
       "       [-1.5465164 ,  1.5227875 ],\n",
       "       [-1.735251  ,  1.7012239 ],\n",
       "       [-0.88818365,  0.89208937],\n",
       "       [-0.7341374 ,  0.7426075 ],\n",
       "       [-1.2152987 ,  1.2075884 ],\n",
       "       [ 0.00737283,  0.01115481]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler.fit(X_train_norm)\n",
    "X_train_scaled = scaler.transform(X_train_norm)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a seed to have reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a classifier equivalent to a logistic regression and we fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 18:20:32.496638: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-10 18:20:33.022663: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb2f82a6e50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(1, input_dim=2, activation='sigmoid')]\n",
    ")\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train_scaled, y_train, epochs=30, batch_size=1, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.9763438],\n",
       "        [ 1.853584 ]], dtype=float32),\n",
       " array([0.03650518], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the probabilities to belong to class 1 for all the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00820595],\n",
       "       [0.17309597],\n",
       "       [0.23926044],\n",
       "       [0.10277784],\n",
       "       [0.01362538],\n",
       "       [0.14149636],\n",
       "       [0.1078079 ],\n",
       "       [0.06255165],\n",
       "       [0.08380368],\n",
       "       [0.28327876],\n",
       "       [0.32420838],\n",
       "       [0.13978043],\n",
       "       [0.0328182 ],\n",
       "       [0.12876213],\n",
       "       [0.00752878],\n",
       "       [0.8423778 ],\n",
       "       [0.93709314],\n",
       "       [0.5308625 ],\n",
       "       [0.9406292 ],\n",
       "       [0.7561799 ],\n",
       "       [0.9648844 ],\n",
       "       [0.7579056 ],\n",
       "       [0.98874676],\n",
       "       [0.9504441 ],\n",
       "       [0.98749506],\n",
       "       [0.9924906 ],\n",
       "       [0.92805845],\n",
       "       [0.89376235],\n",
       "       [0.9695699 ],\n",
       "       [0.5124932 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs = model.predict(X_train_scaled, batch_size=1)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(preds):\n",
    "    c = []\n",
    "    for x in range(len(preds)):\n",
    "        if(preds[x] >= 0.5):\n",
    "            c += [1]\n",
    "        else:\n",
    "            c += [0]\n",
    "    return np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = predict_class(predicted_probs)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the accuracy from the training set. This is not a good practice. We should use a dedicated test set instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
