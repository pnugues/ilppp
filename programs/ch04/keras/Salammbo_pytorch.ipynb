{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier on the *Salammb√¥* Dataset with PyTorch\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "We can read the data from a file with the svmlight format or directly create numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "X_train = np.array(\n",
    "    [[35680, 2217], [42514, 2761], [15162, 990], [35298, 2274],\n",
    "     [29800, 1865], [40255, 2606], [74532, 4805], [37464, 2396],\n",
    "     [31030, 1993], [24843, 1627], [36172, 2375], [39552, 2560],\n",
    "     [72545, 4597], [75352, 4871], [18031, 1119], [36961, 2503],\n",
    "     [43621, 2992], [15694, 1042], [36231, 2487], [29945, 2014],\n",
    "     [40588, 2805], [75255, 5062], [37709, 2643], [30899, 2126],\n",
    "     [25486, 1784], [37497, 2641], [40398, 2766], [74105, 5047],\n",
    "     [76725, 5312], [18317, 1215]\n",
    "     ],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape((-1, 1))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling the Data\n",
    "Scaling and normalizing are usually very significant with neural networks. We use sklean transformers. They consist of two main methods: `fit()` and `transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9980751 , 0.06201605],\n",
       "       [0.99789774, 0.06480679],\n",
       "       [0.9978751 , 0.06515607],\n",
       "       [0.9979313 , 0.06428964],\n",
       "       [0.99804735, 0.06246169],\n",
       "       [0.9979111 , 0.06460207],\n",
       "       [0.9979283 , 0.06433539],\n",
       "       [0.99796116, 0.06382433],\n",
       "       [0.99794376, 0.06409609],\n",
       "       [0.9978623 , 0.06535129],\n",
       "       [0.9978515 , 0.06551746],\n",
       "       [0.9979119 , 0.06458977],\n",
       "       [0.99799836, 0.06324073],\n",
       "       [0.9979172 , 0.06450863],\n",
       "       [0.99807984, 0.06194062],\n",
       "       [0.9977148 , 0.06756528],\n",
       "       [0.9976559 , 0.06843004],\n",
       "       [0.99780315, 0.06624894],\n",
       "       [0.99765235, 0.06848173],\n",
       "       [0.99774593, 0.06710504],\n",
       "       [0.9976205 , 0.06894466],\n",
       "       [0.9977454 , 0.06711297],\n",
       "       [0.9975528 , 0.06991784],\n",
       "       [0.9976413 , 0.06864253],\n",
       "       [0.997559  , 0.06982835],\n",
       "       [0.99752885, 0.07025825],\n",
       "       [0.9976642 , 0.06830881],\n",
       "       [0.99768883, 0.06794866],\n",
       "       [0.99761194, 0.06906894],\n",
       "       [0.9978073 , 0.06618638]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train)\n",
    "X_train_norm = normalizer.transform(X_train)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6832309 , -1.7197777 ],\n",
       "       [ 0.5732034 , -0.56145513],\n",
       "       [ 0.431466  , -0.41648194],\n",
       "       [ 0.7831985 , -0.7761007 ],\n",
       "       [ 1.5094161 , -1.5348101 ],\n",
       "       [ 0.65675384, -0.64642584],\n",
       "       [ 0.76454884, -0.7571132 ],\n",
       "       [ 0.97006804, -0.9692323 ],\n",
       "       [ 0.861154  , -0.85643595],\n",
       "       [ 0.35127246, -0.3354572 ],\n",
       "       [ 0.28376073, -0.2664867 ],\n",
       "       [ 0.66160274, -0.6515314 ],\n",
       "       [ 1.2028158 , -1.2114625 ],\n",
       "       [ 0.6947991 , -0.685208  ],\n",
       "       [ 1.7126973 , -1.7510854 ],\n",
       "       [-0.57151246,  0.58348024],\n",
       "       [-0.9400297 ,  0.9424063 ],\n",
       "       [-0.01873669,  0.03712195],\n",
       "       [-0.96240926,  0.9638616 ],\n",
       "       [-0.37681007,  0.3924546 ],\n",
       "       [-1.1615876 ,  1.1560036 ],\n",
       "       [-0.380167  ,  0.39574805],\n",
       "       [-1.5853077 ,  1.5599338 ],\n",
       "       [-1.031413  ,  1.0306025 ],\n",
       "       [-1.5465164 ,  1.5227875 ],\n",
       "       [-1.735251  ,  1.7012239 ],\n",
       "       [-0.88818365,  0.89208937],\n",
       "       [-0.7341374 ,  0.7426075 ],\n",
       "       [-1.2152987 ,  1.2075884 ],\n",
       "       [ 0.00737283,  0.01115481]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True,with_std=True)\n",
    "scaler.fit(X_train_norm)\n",
    "X_train_scaled = scaler.transform(X_train_norm)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a seed to have reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a classifier equivalent to a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = Variable(torch.Tensor(X_train_scaled).float())\n",
    "y_train = Variable(torch.Tensor(y_train).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_scaled.shape[1]\n",
    "model = Model(input_dim)\n",
    "loss_fn = nn.BCELoss()# binary cross entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    y_train_pred = model(X_train_scaled)\n",
    "    loss = loss_fn(y_train_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.7364,  1.0800]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1754], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the probabilities to belong to class 1 for all the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0099],\n",
       "        [0.1937],\n",
       "        [0.2643],\n",
       "        [0.1168],\n",
       "        [0.0163],\n",
       "        [0.1593],\n",
       "        [0.1224],\n",
       "        [0.0720],\n",
       "        [0.0958],\n",
       "        [0.3107],\n",
       "        [0.3532],\n",
       "        [0.1575],\n",
       "        [0.0384],\n",
       "        [0.1454],\n",
       "        [0.0091],\n",
       "        [0.8579],\n",
       "        [0.9440],\n",
       "        [0.5617],\n",
       "        [0.9472],\n",
       "        [0.7779],\n",
       "        [0.9690],\n",
       "        [0.7795],\n",
       "        [0.9902],\n",
       "        [0.9560],\n",
       "        [0.9891],\n",
       "        [0.9935],\n",
       "        [0.9359],\n",
       "        [0.9048],\n",
       "        [0.9731],\n",
       "        [0.5435]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs = model(X_train_scaled)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(preds):\n",
    "    c = []\n",
    "    for x in range(len(preds)):\n",
    "        if(preds[x] >= 0.5):\n",
    "            c += [1]\n",
    "        else:\n",
    "            c += [0]\n",
    "    return np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = predict_class(predicted_probs)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the accuracy from the training set. This is not a good practice. We should use a dedicated test set instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
