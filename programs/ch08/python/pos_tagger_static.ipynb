{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagger\n",
    "### Pierre Nugues\n",
    "A simple part-of-speech tagger that uses logistic regression. As predictors of a part of speech, we used a window of five words. The code consists of the following parts:\n",
    "* We first read the corpus;\n",
    "* We then extract a matrix of predictors (features), the words, and the response vector, the parts of speech;\n",
    "* We vectorize the matrix, $X$, and the vector, $y$;\n",
    "* We fit the model;\n",
    "* And finally, we evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read the sentences and split the rows of the annotated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = re.split('\\n\\s*\\n', f)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences, where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../../../corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "test_file = '../../../corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "\n",
    "column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "train_sentences = read_sentences(train_file)\n",
    "formatted_corpus = split_rows(train_sentences, column_names)\n",
    "formatted_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to extract the features: A 5-word window centered on the current word. We pad the beginning and the end with dummy symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(sentences, w_size, feature_names, test=False):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    if test:\n",
    "        for sentence in sentences:\n",
    "            X = extract_features_sent(sentence, w_size, feature_names, test)\n",
    "            X_l.extend(X)\n",
    "        return X_l\n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            X, y = extract_features_sent(sentence, w_size, feature_names, test)\n",
    "            X_l.extend(X)\n",
    "            y_l.extend(y)\n",
    "        return X_l, y_l\n",
    "\n",
    "\n",
    "def extract_features_sent(sentence, w_size, feature_names, test=False):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = \"BOS BOS BOS BOS BOS BOS\\n\"\n",
    "    end = \"\\nEOS EOS EOS EOS EOS EOS\"\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    sentence = start + sentence\n",
    "    sentence += end\n",
    "\n",
    "    # Each sentence is a list of rows\n",
    "    sentence = sentence.splitlines()\n",
    "    padded_sentence = list()\n",
    "    for line in sentence:\n",
    "        line = line.split()\n",
    "        padded_sentence.append(line)\n",
    "    # print(padded_sentence)\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][1].lower())\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        if not test:\n",
    "            # The classes are stored in a list\n",
    "            y.append(padded_sentence[i + w_size][4])\n",
    "    if test:\n",
    "        return X\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features from a partial data set to have a shorter training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2']\n",
    "\n",
    "print(\"Extracting the features...\")\n",
    "w_size = 2\n",
    "# We reduce the data set for the demonstration\n",
    "train_sentences = train_sentences[:int(len(train_sentences)/5)]\n",
    "\n",
    "X_dict, y_symbols = extract_features(train_sentences, w_size, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the features to check it matches Table 8.1 in my book (second edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_dict[48759:48790])\n",
    "y_symbols[48759:48790]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the feature matrix and carry out a one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to encode the class labels into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_symbols)\n",
    "print('Response classes:', le.classes_)\n",
    "y = le.transform(y_symbols)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier.fit(X, y)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "formatted_test_corpus = split_rows(test_sentences, column_names)\n",
    "formatted_test_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the features of the test corpus and vectorize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we carry out a pos tag prediction and we report the per tag error\n",
    "# This is done for the whole corpus without regard for the sentence structure\n",
    "\n",
    "X_test_dict, y_test_symbols = extract_features(test_sentences, w_size, feature_names)\n",
    "# Vectorize the test set and one-hot encoding\n",
    "X_test = vec.transform(X_test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we predict the test set and measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Predicting the POS in the test set...\")\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "y_test_predicted_symbols = le.inverse_transform(y_test_predicted)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (classifier, metrics.classification_report(y_test_symbols, y_test_predicted_symbols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"\"\"1\\tthat\n",
    "2\\tround\n",
    "3\\ttable\n",
    "4\\tmight\n",
    "5\\tcollapse\n",
    "6\\t.\"\"\"\n",
    "\n",
    "sentence2 = \"\"\"1\\tthe\n",
    "2\\tman\n",
    "3\\tcan\n",
    "4\\tswim\n",
    "5\\t.\"\"\"\n",
    "\n",
    "my_sentences = [sentence1, sentence2]\n",
    "\n",
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    X_s_dict= extract_features([sentence], w_size, feature_names, True)\n",
    "    X_s = vec.transform(X_s_dict)\n",
    "    y_s = classifier.predict(X_s)\n",
    "    y_symb = le.inverse_transform(y_s)\n",
    "    print(y_symb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
