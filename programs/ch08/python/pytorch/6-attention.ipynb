{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa3ccdd9-ace9-4c89-ad8e-0faebeec5c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Attention mechanism and its implementation in PyTorch\n",
    "\n",
    "Computing self-attention of a sentence with GloVe embeddings and the `MultiheadAttention` class with PyTorch\n",
    "\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba630853-00e0-4b64-ab86-01185c2deb0e",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d21c1d0-b06a-4340-bf3d-419a6edaf327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119bcd1-e5af-4b3a-bd87-45276f71c7db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Noncontextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5741d7ec-e588-4a6d-9cba-f8eae75f4fad",
   "metadata": {},
   "source": [
    "We load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d84fbb-b15e-45c0-bba8-6074ec0bb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.FloatTensor(list(map(float, values[1:])))\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e0a3c6-6e89-4500-b555-fb40efb5bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/glove.6B.50d.txt'\n",
    "embeddings_dict = read_embeddings(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ba6dc1-a704-4e08-b89b-fc5325ebcf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5213,  0.1052,  0.3816, -0.5080,  0.0324, -0.1348, -1.2474,  0.7981,\n",
       "         0.8469, -1.1010,  0.8874,  1.3749,  0.4293,  0.6572, -0.2636, -0.4176,\n",
       "        -0.4885,  0.9106, -1.7158, -0.4380,  0.7839,  0.1964, -0.4066, -0.5397,\n",
       "         0.8244, -1.7434,  0.1428,  0.2804,  1.1688,  0.1690,  2.2271, -0.5827,\n",
       "        -0.4572,  0.6281,  0.5444,  0.2846,  0.4448, -0.5534, -0.3649, -0.0164,\n",
       "         0.4088, -0.8715,  1.5513, -0.8070, -0.1004, -0.2846, -0.3322, -0.5061,\n",
       "         0.4827, -0.6620])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['ship']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9f1dd-07e7-4995-abea-8c2fe202e16e",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5c648e3-c5f9-4e70-82f3-1b138400c1af",
   "metadata": {},
   "source": [
    "Let us compute the cosine similarity of the words in a sentence:\n",
    "> I must go back to my ship and to my crew\n",
    "\n",
    "_Odyssey_, book I \n",
    "\n",
    "Remember that:\n",
    "$$\\cos(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{||\\mathbf{u}|| \\cdot ||\\mathbf{v} ||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02456cd9-bc86-4ccb-ad68-e4124cb16e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_odyssey = 'I must go back to my ship and to my crew'\n",
    "sentence_amazon = 'We process and ship your order'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0851862-27f2-4fd9-98ed-e9aa64d2dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'must', 'go', 'back', 'to', 'my', 'ship', 'and', 'to', 'my', 'crew']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_a = sentence_amazon.lower().split()\n",
    "words_o = sentence_odyssey.lower().split()\n",
    "words_o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600768d3-408b-48cc-994d-710fdb0c45df",
   "metadata": {},
   "source": [
    "We build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef726a70-39fb-4bc9-826b-fa4efa6f889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix(words):\n",
    "    embeddings_seq = []\n",
    "    for word in words:\n",
    "        embeddings_seq += [embeddings_dict[word]]\n",
    "    embeddings_seq = torch.stack(embeddings_seq)\n",
    "    return embeddings_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4dbf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_seq_a = embedding_matrix(words_a)\n",
    "embeddings_seq_o = embedding_matrix(words_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97d5907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 50])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_seq_o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "807f131d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n",
       "        -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_seq_o[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049e121-9f7b-4392-a4db-e99e06f25bf4",
   "metadata": {},
   "source": [
    "We compute the attention scores as the pairwise cosines of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c68fa0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_cos_scores(embeddings_seq):\n",
    "    E_normed = embeddings_seq/torch.norm(embeddings_seq_o, dim=-1).reshape(-1, 1)\n",
    "    attn_scores_cos = E_normed @ E_normed.T\n",
    "    return attn_scores_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6db4f3c-57c0-4e4d-a66e-5ecb94cf5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cos_scores(words):\n",
    "    embeddings = embedding_matrix(words)\n",
    "    attn_scores_cos = attn_cos_scores(embeddings)\n",
    "    print('\\t', end='')\n",
    "    for i in range(len(words)):\n",
    "        print(words[i], end='\\t')\n",
    "    print()\n",
    "\n",
    "    for i in range(attn_scores_cos.shape[0]):\n",
    "        print(words[i], end='\\t')\n",
    "        for j in range(attn_scores_cos.shape[1]):\n",
    "            print(f\"{attn_scores_cos[i,j]:.2f}\", end='\\t')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76984ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ti\tmust\tgo\tback\tto\tmy\tship\tand\tto\tmy\tcrew\t\n",
      "i\t1.00\t0.75\t0.86\t0.76\t0.73\t0.90\t0.35\t0.65\t0.73\t0.90\t0.42\t\n",
      "must\t0.75\t1.00\t0.85\t0.68\t0.87\t0.69\t0.42\t0.69\t0.87\t0.69\t0.45\t\n",
      "go\t0.86\t0.85\t1.00\t0.84\t0.84\t0.81\t0.41\t0.68\t0.84\t0.81\t0.49\t\n",
      "back\t0.76\t0.68\t0.84\t1.00\t0.83\t0.76\t0.49\t0.77\t0.83\t0.76\t0.51\t\n",
      "to\t0.73\t0.87\t0.84\t0.83\t1.00\t0.68\t0.54\t0.86\t1.00\t0.68\t0.51\t\n",
      "my\t0.90\t0.69\t0.81\t0.76\t0.68\t1.00\t0.38\t0.63\t0.68\t1.00\t0.44\t\n",
      "ship\t0.35\t0.42\t0.41\t0.49\t0.54\t0.38\t1.00\t0.46\t0.54\t0.38\t0.78\t\n",
      "and\t0.65\t0.69\t0.68\t0.77\t0.86\t0.63\t0.46\t1.00\t0.86\t0.63\t0.49\t\n",
      "to\t0.73\t0.87\t0.84\t0.83\t1.00\t0.68\t0.54\t0.86\t1.00\t0.68\t0.51\t\n",
      "my\t0.90\t0.69\t0.81\t0.76\t0.68\t1.00\t0.38\t0.63\t0.68\t1.00\t0.44\t\n",
      "crew\t0.42\t0.45\t0.49\t0.51\t0.51\t0.44\t0.78\t0.49\t0.51\t0.44\t1.00\t\n"
     ]
    }
   ],
   "source": [
    "print_cos_scores(words_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c498b-1650-403a-9ba1-780ba3ef4dd5",
   "metadata": {},
   "source": [
    "## Contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958548e0-5560-4c09-bb13-9731cccf72fa",
   "metadata": {},
   "source": [
    "We design a new vector representation for _ship_ so that it receives an influence from _crew_ and the other words of its context. This influence will depend on the embeddings from te context. Let us use the cosine similarities as attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cedf865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3466, 0.4178, 0.4068, 0.4853, 0.5401, 0.3791, 1.0000, 0.4586, 0.5401,\n",
       "        0.3791, 0.7848])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_cos_scores(embeddings_seq_o)[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca41244-cf33-4d0c-9bd9-d1b705f45b1f",
   "metadata": {},
   "source": [
    "We compute the new embeddings as the sum of the noncontextual embeddings weighted by the cosine similarity. We have contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2700f2-3bbe-4911-9e73-bc32ad1cf082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.2289,   0.6422,   1.4712,  -2.3538,   2.2414,  -0.4237,  -4.1052,\n",
       "          2.6216,   0.1719,  -2.4324,   1.3882,   3.7241,  -1.9721,   1.1893,\n",
       "          2.2511,   0.9502,  -0.7646,   1.0289,  -3.0553,  -3.6306,   0.8305,\n",
       "          2.9299,   1.3221,  -0.7092,   2.9745, -10.5959,  -1.3168,   0.2059,\n",
       "          3.5457,  -2.7711,  18.2672,   2.4817,  -3.5887,   0.3297,   1.2718,\n",
       "          0.6539,   1.5873,   0.0195,   0.7724,  -1.4620,  -0.2067,  -1.2464,\n",
       "          2.1504,  -0.1811,  -0.5026,  -0.2888,  -0.5060,  -1.9676,  -0.0605,\n",
       "         -0.6725])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings_ship = (0.35 * embeddings_dict['i'] + \n",
    "                  0.42 * embeddings_dict['must'] + \n",
    "                  0.41 * embeddings_dict['go'] +\n",
    "                  0.49 * embeddings_dict['back'] +\n",
    "                  0.54 * embeddings_dict['to'] + \n",
    "                  0.38 * embeddings_dict['my'] +\n",
    "                  1.00 * embeddings_dict['ship'] +\n",
    "                  0.46 * embeddings_dict['and'] +\n",
    "                  0.54 * embeddings_dict['to'] +\n",
    "                  0.38 * embeddings_dict['my'] +\n",
    "                  0.78 * embeddings_dict['crew'])\n",
    "new_embeddings_ship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "135c0613-cac0-4fbc-abc0-30f0727a79a1",
   "metadata": {},
   "source": [
    "Exact computation with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82630b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.2319e+00,  6.4082e-01,  1.4718e+00, -2.3434e+00,  2.2358e+00,\n",
       "        -4.1877e-01, -4.1002e+00,  2.6211e+00,  1.8010e-01, -2.4360e+00,\n",
       "         1.3923e+00,  3.7188e+00, -1.9603e+00,  1.1980e+00,  2.2394e+00,\n",
       "         9.3763e-01, -7.7049e-01,  1.0349e+00, -3.0615e+00, -3.6259e+00,\n",
       "         8.3401e-01,  2.9281e+00,  1.3165e+00, -7.1303e-01,  2.9667e+00,\n",
       "        -1.0567e+01, -1.3099e+00,  2.0283e-01,  3.5362e+00, -2.7571e+00,\n",
       "         1.8220e+01,  2.4698e+00, -3.5804e+00,  3.2604e-01,  1.2760e+00,\n",
       "         6.5701e-01,  1.5889e+00,  1.1571e-02,  7.6620e-01, -1.4560e+00,\n",
       "        -2.0362e-01, -1.2484e+00,  2.1550e+00, -1.8767e-01, -5.0253e-01,\n",
       "        -2.9128e-01, -5.1006e-01, -1.9596e+00, -5.8853e-02, -6.7380e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn_cos_scores(embeddings_seq_o) @ embeddings_seq_o)[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8b7cf-0ef8-46dd-96d2-e94601dcdf45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self-attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe38dcb-887e-4b92-871e-011918b76140",
   "metadata": {},
   "source": [
    "Vaswani et al. (2017) defined attention as:\n",
    "$$\n",
    "\\text{Attention}({Q}, {K}, {Q}) = \\text{softmax}(\\frac{{Q}  {K}^\\intercal}{\\sqrt{d_k}})  {V},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "{Q} &=& {X} {W}_Q,   \\\\\n",
    "{K} &=& {X} {W}_K , \\\\\n",
    "{V} &=& {X} {W}_V.\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "and ${X}$ represents complete input sequence (all the tokens).\n",
    "\n",
    "$d_k$ is the dimension of the input and $\\sqrt{d_k}$ a scaling factor. The $\\text{softmax}$ function is defined as:\n",
    "$$\n",
    "\\text{softmax}(x_1, x_2, ..., x_j, ..., x_n) = (\\frac{e^{x_1}}{\\sum_{i=1}^n e^{x_i}}, \\frac{e^{x_2}}{\\sum_{i=1}^n e^{x_i}}, ..., \\frac{e^{x_j}}{\\sum_{i=1}^n e^{x_i}}, ..., \\frac{e^{x_n}}{\\sum_{i=1}^n e^{x_i}})\n",
    "$$\n",
    "\n",
    "We omit the weight matrices and we use the same embeddings for ${Q}$, ${K}$, and ${Q}$: GloVe embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77bac5b7-2636-405a-ad9d-5ce0aec3f541",
   "metadata": {},
   "source": [
    "For the matrix above, self attention, $\\text{softmax}(\\frac{{Q}  {K}^\\intercal}{\\sqrt{d_k}})$,  for _ship_ yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc144c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dk = embeddings_dict['i'].size()[0]\n",
    "dk = torch.tensor(dk)\n",
    "dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd81a119-112f-4eb1-9f36-7fa0514ac14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_o = F.softmax(embeddings_seq_o @ embeddings_seq_o.T/torch.sqrt(dk), dim=-1)\n",
    "attn_scores_o[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9658c8f-2804-4f0b-8c6d-c0cd5e10e0c2",
   "metadata": {},
   "source": [
    "The scaled and normalized attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf831af2-f40f-4512-a661-e300c467b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attn_scores(words):\n",
    "    embeddings = embedding_matrix(words)\n",
    "    sent_length, dk = embeddings.size()\n",
    "    attn_scores = F.softmax(embeddings @ embeddings.T/torch.sqrt(torch.tensor(dk)), dim=-1)\n",
    "    print('\\t', end='')\n",
    "    for i in range(sent_length):\n",
    "        print(words[i], end='\\t')\n",
    "    print()\n",
    "    for i in range(sent_length):\n",
    "        print(words[i], end='\\t')\n",
    "        for j in range(sent_length):\n",
    "            print(f\"{attn_scores[i,j]:.2f}\", end='\\t')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f977f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ti\tmust\tgo\tback\tto\tmy\tship\tand\tto\tmy\tcrew\t\n",
      "i\t0.36\t0.05\t0.07\t0.05\t0.04\t0.19\t0.01\t0.02\t0.04\t0.19\t0.01\t\n",
      "must\t0.14\t0.20\t0.10\t0.06\t0.11\t0.10\t0.03\t0.05\t0.11\t0.10\t0.02\t\n",
      "go\t0.18\t0.09\t0.14\t0.09\t0.08\t0.13\t0.02\t0.04\t0.08\t0.13\t0.02\t\n",
      "back\t0.14\t0.05\t0.09\t0.19\t0.08\t0.12\t0.03\t0.06\t0.08\t0.12\t0.03\t\n",
      "to\t0.11\t0.11\t0.09\t0.09\t0.15\t0.08\t0.04\t0.07\t0.15\t0.08\t0.03\t\n",
      "my\t0.19\t0.03\t0.05\t0.04\t0.03\t0.29\t0.01\t0.02\t0.03\t0.29\t0.01\t\n",
      "ship\t0.03\t0.03\t0.03\t0.04\t0.05\t0.03\t0.55\t0.03\t0.05\t0.03\t0.13\t\n",
      "and\t0.10\t0.08\t0.07\t0.10\t0.12\t0.09\t0.04\t0.15\t0.12\t0.09\t0.04\t\n",
      "to\t0.11\t0.11\t0.09\t0.09\t0.15\t0.08\t0.04\t0.07\t0.15\t0.08\t0.03\t\n",
      "my\t0.19\t0.03\t0.05\t0.04\t0.03\t0.29\t0.01\t0.02\t0.03\t0.29\t0.01\t\n",
      "crew\t0.06\t0.05\t0.05\t0.06\t0.05\t0.06\t0.21\t0.04\t0.05\t0.06\t0.31\t\n"
     ]
    }
   ],
   "source": [
    "print_attn_scores(words_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894ff99-35da-460d-8121-b69492a26e21",
   "metadata": {},
   "source": [
    "For _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e556eebb-36bd-4046-90a2-bdc0df48523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_o[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb44a4-ba75-400e-96da-da4f197b51c8",
   "metadata": {},
   "source": [
    "We have the weights of 55% for _ship_ and 13% for _crew_, the rest from the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624ca6b-c824-4e1c-a23e-2e05222d3d26",
   "metadata": {},
   "source": [
    "And the new contextual embedding is for _ship_ is a linear combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb9d28fa-a98a-4579-9ff6-4ceba203dcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0442,  0.0966,  0.3467, -0.4238,  0.2203, -0.0956, -0.9915,  0.6637,\n",
       "         0.4368, -0.7943,  0.5639,  0.9838,  0.0240,  0.5066,  0.0732, -0.1740,\n",
       "        -0.3322,  0.5614, -1.1613, -0.5717,  0.4356,  0.4120, -0.0659, -0.3336,\n",
       "         0.6579, -1.7421, -0.0344,  0.1440,  0.8547, -0.1430,  2.6614, -0.0553,\n",
       "        -0.5376,  0.3057,  0.4068,  0.2231,  0.3959, -0.2940, -0.1163, -0.1340,\n",
       "         0.1709, -0.5332,  0.9552, -0.4178, -0.1058, -0.1715, -0.2251, -0.3923,\n",
       "         0.2098, -0.3625])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_ship = (0.03 * embeddings_dict['i'] + \n",
    "                  0.03 * embeddings_dict['must'] + \n",
    "                  0.03 * embeddings_dict['go'] +\n",
    "                  0.04 * embeddings_dict['back'] +\n",
    "                  0.05 * embeddings_dict['to'] + \n",
    "                  0.03 * embeddings_dict['my'] +\n",
    "                  0.55 * embeddings_dict['ship'] +\n",
    "                  0.03 * embeddings_dict['and'] +\n",
    "                  0.05 * embeddings_dict['to'] +\n",
    "                  0.03 * embeddings_dict['my'] +\n",
    "                  0.13 * embeddings_dict['crew'])\n",
    "self_attention_ship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc714db7-aa8d-41de-b844-0e2299ae829d",
   "metadata": {},
   "source": [
    "Exact and complete computation of the whole matrix with torch of \n",
    "$$\n",
    "\\text{softmax}(\\frac{{Q}  {K}^\\intercal}{\\sqrt{d_k}})  {V} :\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ea8976f-3ee2-406d-9318-2ca0eecb5616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_attention_output_o = attn_scores_o @ embeddings_seq_o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1afaa573-f2bd-49d2-9cb5-dced32b4c5a3",
   "metadata": {},
   "source": [
    "The contextual embeddings for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "069655f9-bd3a-4b04-903a-c65ae881baec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0387,  0.1033,  0.3426, -0.4320,  0.2237, -0.0958, -0.9926,  0.6662,\n",
       "         0.4424, -0.7942,  0.5638,  0.9921,  0.0205,  0.5082,  0.0743, -0.1773,\n",
       "        -0.3408,  0.5675, -1.1545, -0.5718,  0.4288,  0.4191, -0.0658, -0.3339,\n",
       "         0.6682, -1.7473, -0.0485,  0.1531,  0.8642, -0.1447,  2.6571, -0.0545,\n",
       "        -0.5343,  0.3160,  0.4041,  0.2277,  0.3958, -0.2916, -0.1126, -0.1385,\n",
       "         0.1744, -0.5375,  0.9499, -0.4145, -0.1039, -0.1755, -0.2213, -0.3995,\n",
       "         0.2119, -0.3610])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_output_o[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011b9ef",
   "metadata": {},
   "source": [
    "We can now write a `self_attention` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_seq):\n",
    "    dk = torch.tensor(input_seq.size()[-1])\n",
    "    attn_scores = F.softmax(input_seq @ input_seq.T/torch.sqrt(dk), dim=-1)\n",
    "    attn_output = attn_scores @ input_seq\n",
    "    return attn_output, attn_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fc9eaba",
   "metadata": {},
   "source": [
    "The word _ship_ in another context: _We process and ship your order_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70859d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output_a, attn_scores_a = self_attention(embeddings_seq_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaaf4a",
   "metadata": {},
   "source": [
    "Attention scores for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8bfb2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0431, 0.0258, 0.0419, 0.7811, 0.0490, 0.0590])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a45d66fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\twe\tprocess\tand\tship\tyour\torder\t\n",
      "we\t0.61\t0.06\t0.06\t0.02\t0.20\t0.05\t\n",
      "process\t0.17\t0.50\t0.08\t0.03\t0.11\t0.11\t\n",
      "and\t0.22\t0.12\t0.30\t0.08\t0.15\t0.13\t\n",
      "ship\t0.04\t0.03\t0.04\t0.78\t0.05\t0.06\t\n",
      "your\t0.14\t0.03\t0.03\t0.02\t0.74\t0.04\t\n",
      "order\t0.16\t0.13\t0.10\t0.09\t0.18\t0.34\t\n"
     ]
    }
   ],
   "source": [
    "print_attn_scores(words_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963dec4",
   "metadata": {},
   "source": [
    "The new contextual embeddings for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37a2b793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2758,  0.1034,  0.2720, -0.4776,  0.1746, -0.1060, -0.9901,  0.6328,\n",
       "         0.6967, -0.8847,  0.7106,  1.2264,  0.2491,  0.5023, -0.1277, -0.2361,\n",
       "        -0.3709,  0.6545, -1.2587, -0.5332,  0.6681,  0.1687, -0.2567, -0.4218,\n",
       "         0.6960, -1.7077, -0.0052,  0.1572,  1.0763,  0.0410,  2.5467, -0.3418,\n",
       "        -0.5414,  0.4175,  0.4147,  0.2666,  0.3770, -0.4228, -0.2462, -0.0377,\n",
       "         0.3202, -0.7298,  1.2020, -0.5636, -0.0899, -0.1845, -0.2390, -0.4307,\n",
       "         0.3828, -0.4905])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output_a[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86dd9e67-74f1-4188-b5cc-8fee780a2a1d",
   "metadata": {},
   "source": [
    "## PyTorch implementation\n",
    " \n",
    "PyTorch has an implementation of self-attention encapsulated in the `MultiheadAttention` class. Before going to the attention module, the query, key value, goes through a linear layer. The output also goes through a linear layer. These three layers are initialized with Xavier's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69c3124e-0197-4713-a837-c880734509bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "att_layer = MultiheadAttention(50, \n",
    "                               1,\n",
    "                               bias=False,\n",
    "                               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "396c1d82-c5a2-4fee-be93-54229835eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn_output, attn_scores) = att_layer(embeddings_seq_o, embeddings_seq_o, embeddings_seq_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c42892-b9cb-4e06-a63f-b1b9f79506db",
   "metadata": {},
   "source": [
    "The attention score for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6164627-82f0-4f9d-8c55-e48d05a6f155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1005, 0.0725, 0.0798, 0.0987, 0.0696, 0.0930, 0.0930, 0.0785, 0.0696,\n",
       "        0.0930, 0.1516], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf8e1e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3139e-01, -3.6727e-01, -1.8204e-01, -2.4207e-01,  1.4971e-01,\n",
       "         -1.1306e-01,  2.4283e-01, -2.1270e-01,  3.5660e-01,  5.4594e-01,\n",
       "          1.6515e-01, -1.4105e-01,  3.6244e-02, -8.5999e-02, -1.8266e-01,\n",
       "          3.8833e-01, -3.4812e-01, -2.7353e-01, -3.2677e-01, -6.7322e-01,\n",
       "          4.3488e-02, -6.8552e-02,  3.9725e-01, -2.1517e-01,  1.5552e-02,\n",
       "          1.5287e-01,  1.8118e-01, -2.8809e-01,  2.4120e-01, -2.8585e-01,\n",
       "         -7.9411e-02, -1.3777e-01, -1.9382e-01, -1.9365e-03, -4.6244e-02,\n",
       "          1.1650e-01,  1.3885e-01, -2.8046e-01,  1.0420e-01, -5.4482e-01,\n",
       "          2.2524e-01, -2.3087e-01, -3.8066e-01,  2.4418e-01, -4.7407e-01,\n",
       "         -4.1701e-01, -3.1227e-01, -1.4631e-01,  1.1491e-01,  2.2806e-01],\n",
       "        [-1.2243e-01, -3.8173e-01, -1.9070e-01, -2.4583e-01,  1.4383e-01,\n",
       "         -1.1336e-01,  2.5795e-01, -2.0619e-01,  3.6367e-01,  5.5137e-01,\n",
       "          1.5844e-01, -1.5577e-01,  4.2192e-02, -7.6187e-02, -1.7239e-01,\n",
       "          3.9157e-01, -3.4213e-01, -2.7343e-01, -3.1634e-01, -6.7960e-01,\n",
       "          2.9100e-02, -6.8450e-02,  3.9147e-01, -2.0506e-01,  9.7319e-04,\n",
       "          1.4940e-01,  1.5681e-01, -2.9494e-01,  2.0915e-01, -2.6950e-01,\n",
       "         -7.0177e-02, -1.3362e-01, -2.0169e-01, -2.9331e-03, -4.8276e-02,\n",
       "          1.0124e-01,  1.5461e-01, -2.7417e-01,  1.1244e-01, -5.5849e-01,\n",
       "          2.1942e-01, -2.3330e-01, -3.7037e-01,  2.4512e-01, -4.7383e-01,\n",
       "         -4.1204e-01, -3.0915e-01, -1.5213e-01,  1.1016e-01,  2.2939e-01],\n",
       "        [-1.2899e-01, -3.7134e-01, -1.8668e-01, -2.4145e-01,  1.4440e-01,\n",
       "         -1.1288e-01,  2.4676e-01, -2.0753e-01,  3.5925e-01,  5.5026e-01,\n",
       "          1.6249e-01, -1.4434e-01,  3.7590e-02, -8.1578e-02, -1.7633e-01,\n",
       "          3.8625e-01, -3.4737e-01, -2.7118e-01, -3.2282e-01, -6.7462e-01,\n",
       "          3.9361e-02, -6.6365e-02,  3.9412e-01, -2.0808e-01,  6.3444e-03,\n",
       "          1.5413e-01,  1.7411e-01, -2.8897e-01,  2.2835e-01, -2.7795e-01,\n",
       "         -7.5391e-02, -1.3379e-01, -1.9486e-01,  6.0709e-04, -4.8514e-02,\n",
       "          1.0943e-01,  1.4606e-01, -2.7853e-01,  1.0580e-01, -5.4574e-01,\n",
       "          2.2516e-01, -2.3102e-01, -3.7619e-01,  2.4571e-01, -4.7365e-01,\n",
       "         -4.1181e-01, -3.0914e-01, -1.4548e-01,  1.1722e-01,  2.2970e-01],\n",
       "        [-1.2532e-01, -3.8104e-01, -1.9713e-01, -2.4097e-01,  1.3821e-01,\n",
       "         -1.1234e-01,  2.6081e-01, -2.0350e-01,  3.6538e-01,  5.5583e-01,\n",
       "          1.5548e-01, -1.5405e-01,  3.7609e-02, -7.1963e-02, -1.7127e-01,\n",
       "          3.8575e-01, -3.5006e-01, -2.7141e-01, -3.1415e-01, -6.8161e-01,\n",
       "          2.8656e-02, -6.3692e-02,  3.9305e-01, -1.9951e-01, -8.9595e-03,\n",
       "          1.5420e-01,  1.5680e-01, -2.9919e-01,  2.0544e-01, -2.7190e-01,\n",
       "         -7.0532e-02, -1.3118e-01, -1.9647e-01,  6.6928e-03, -5.1998e-02,\n",
       "          9.5971e-02,  1.5143e-01, -2.7622e-01,  1.0830e-01, -5.5645e-01,\n",
       "          2.2565e-01, -2.3491e-01, -3.7200e-01,  2.4745e-01, -4.7445e-01,\n",
       "         -3.9936e-01, -3.1224e-01, -1.4526e-01,  1.2103e-01,  2.3069e-01],\n",
       "        [-1.1872e-01, -3.8749e-01, -1.9723e-01, -2.4504e-01,  1.3588e-01,\n",
       "         -1.1187e-01,  2.6767e-01, -1.9961e-01,  3.6633e-01,  5.5662e-01,\n",
       "          1.5316e-01, -1.6285e-01,  4.4689e-02, -6.8721e-02, -1.6792e-01,\n",
       "          3.8675e-01, -3.4377e-01, -2.6867e-01, -3.0949e-01, -6.8253e-01,\n",
       "          2.0658e-02, -6.5193e-02,  3.9139e-01, -1.9497e-01, -1.5048e-02,\n",
       "          1.5113e-01,  1.4611e-01, -3.0026e-01,  1.8921e-01, -2.5971e-01,\n",
       "         -6.3233e-02, -1.3030e-01, -2.0304e-01,  3.5858e-03, -5.2154e-02,\n",
       "          9.1742e-02,  1.6331e-01, -2.7174e-01,  1.1290e-01, -5.6251e-01,\n",
       "          2.2203e-01, -2.3501e-01, -3.6549e-01,  2.4868e-01, -4.7534e-01,\n",
       "         -4.0042e-01, -3.0669e-01, -1.4755e-01,  1.1470e-01,  2.3122e-01],\n",
       "        [-1.3123e-01, -3.6787e-01, -1.8399e-01, -2.4103e-01,  1.4744e-01,\n",
       "         -1.1393e-01,  2.4266e-01, -2.0954e-01,  3.5702e-01,  5.4866e-01,\n",
       "          1.6314e-01, -1.3979e-01,  3.6078e-02, -8.5710e-02, -1.8112e-01,\n",
       "          3.8643e-01, -3.4975e-01, -2.7304e-01, -3.2557e-01, -6.7348e-01,\n",
       "          4.2328e-02, -6.6433e-02,  3.9627e-01, -2.1239e-01,  1.3261e-02,\n",
       "          1.5376e-01,  1.7976e-01, -2.8823e-01,  2.3835e-01, -2.8344e-01,\n",
       "         -7.8410e-02, -1.3764e-01, -1.9333e-01,  1.0618e-03, -4.6822e-02,\n",
       "          1.1367e-01,  1.4053e-01, -2.7968e-01,  1.0413e-01, -5.4337e-01,\n",
       "          2.2645e-01, -2.3133e-01, -3.8024e-01,  2.4596e-01, -4.7520e-01,\n",
       "         -4.1360e-01, -3.1173e-01, -1.4531e-01,  1.1541e-01,  2.2919e-01],\n",
       "        [-1.1524e-01, -3.9791e-01, -1.9575e-01, -2.3163e-01,  1.2859e-01,\n",
       "         -1.2078e-01,  2.7333e-01, -1.8279e-01,  3.6454e-01,  5.6518e-01,\n",
       "          1.3687e-01, -1.5837e-01,  3.5673e-02, -7.3837e-02, -1.7647e-01,\n",
       "          3.7916e-01, -3.6373e-01, -2.7898e-01, -2.9318e-01, -6.9482e-01,\n",
       "         -5.3024e-03, -5.2031e-02,  4.0743e-01, -1.9300e-01, -1.6694e-02,\n",
       "          1.5669e-01,  1.2921e-01, -3.1815e-01,  1.8205e-01, -2.5373e-01,\n",
       "         -6.2914e-02, -1.4809e-01, -2.1531e-01,  2.8318e-02, -4.7804e-02,\n",
       "          7.7579e-02,  1.4913e-01, -2.7360e-01,  1.0919e-01, -5.6712e-01,\n",
       "          2.3121e-01, -2.4395e-01, -3.7268e-01,  2.5537e-01, -4.8346e-01,\n",
       "         -3.7155e-01, -3.2190e-01, -1.4757e-01,  1.0501e-01,  2.3137e-01],\n",
       "        [-1.1573e-01, -3.9274e-01, -2.0092e-01, -2.4417e-01,  1.3250e-01,\n",
       "         -1.1079e-01,  2.7446e-01, -1.9727e-01,  3.6994e-01,  5.5931e-01,\n",
       "          1.5156e-01, -1.6987e-01,  4.5542e-02, -6.3559e-02, -1.6290e-01,\n",
       "          3.8787e-01, -3.4310e-01, -2.6897e-01, -3.0543e-01, -6.8601e-01,\n",
       "          1.5275e-02, -6.5035e-02,  3.9016e-01, -1.9062e-01, -2.3427e-02,\n",
       "          1.5124e-01,  1.3749e-01, -3.0489e-01,  1.7615e-01, -2.5432e-01,\n",
       "         -5.9628e-02, -1.2696e-01, -2.0514e-01,  3.6713e-03, -5.4046e-02,\n",
       "          8.4085e-02,  1.6661e-01, -2.7100e-01,  1.1449e-01, -5.6766e-01,\n",
       "          2.2140e-01, -2.3589e-01, -3.6169e-01,  2.4786e-01, -4.7270e-01,\n",
       "         -3.9600e-01, -3.0653e-01, -1.4860e-01,  1.1590e-01,  2.3194e-01],\n",
       "        [-1.1872e-01, -3.8749e-01, -1.9723e-01, -2.4504e-01,  1.3588e-01,\n",
       "         -1.1187e-01,  2.6767e-01, -1.9961e-01,  3.6633e-01,  5.5662e-01,\n",
       "          1.5316e-01, -1.6285e-01,  4.4689e-02, -6.8721e-02, -1.6792e-01,\n",
       "          3.8675e-01, -3.4377e-01, -2.6867e-01, -3.0949e-01, -6.8253e-01,\n",
       "          2.0658e-02, -6.5193e-02,  3.9139e-01, -1.9497e-01, -1.5048e-02,\n",
       "          1.5113e-01,  1.4611e-01, -3.0026e-01,  1.8921e-01, -2.5971e-01,\n",
       "         -6.3233e-02, -1.3030e-01, -2.0304e-01,  3.5858e-03, -5.2154e-02,\n",
       "          9.1742e-02,  1.6331e-01, -2.7174e-01,  1.1290e-01, -5.6251e-01,\n",
       "          2.2203e-01, -2.3501e-01, -3.6549e-01,  2.4868e-01, -4.7534e-01,\n",
       "         -4.0042e-01, -3.0669e-01, -1.4755e-01,  1.1470e-01,  2.3122e-01],\n",
       "        [-1.3123e-01, -3.6787e-01, -1.8399e-01, -2.4103e-01,  1.4744e-01,\n",
       "         -1.1393e-01,  2.4266e-01, -2.0954e-01,  3.5702e-01,  5.4866e-01,\n",
       "          1.6314e-01, -1.3979e-01,  3.6078e-02, -8.5710e-02, -1.8112e-01,\n",
       "          3.8643e-01, -3.4975e-01, -2.7304e-01, -3.2557e-01, -6.7348e-01,\n",
       "          4.2328e-02, -6.6433e-02,  3.9627e-01, -2.1239e-01,  1.3261e-02,\n",
       "          1.5376e-01,  1.7976e-01, -2.8823e-01,  2.3835e-01, -2.8344e-01,\n",
       "         -7.8410e-02, -1.3764e-01, -1.9333e-01,  1.0618e-03, -4.6822e-02,\n",
       "          1.1367e-01,  1.4053e-01, -2.7968e-01,  1.0413e-01, -5.4337e-01,\n",
       "          2.2645e-01, -2.3133e-01, -3.8024e-01,  2.4596e-01, -4.7520e-01,\n",
       "         -4.1360e-01, -3.1173e-01, -1.4531e-01,  1.1541e-01,  2.2919e-01],\n",
       "        [-1.1744e-01, -3.9324e-01, -1.9168e-01, -2.3369e-01,  1.3599e-01,\n",
       "         -1.1955e-01,  2.6473e-01, -1.9045e-01,  3.6550e-01,  5.6036e-01,\n",
       "          1.4597e-01, -1.5615e-01,  3.5597e-02, -7.8681e-02, -1.7279e-01,\n",
       "          3.8890e-01, -3.5644e-01, -2.8340e-01, -3.0177e-01, -6.9018e-01,\n",
       "          6.3864e-03, -5.8210e-02,  4.0253e-01, -2.0193e-01, -4.2790e-03,\n",
       "          1.5425e-01,  1.3863e-01, -3.1049e-01,  1.9611e-01, -2.6008e-01,\n",
       "         -6.8193e-02, -1.4517e-01, -2.1441e-01,  1.4726e-02, -4.5714e-02,\n",
       "          8.4083e-02,  1.4570e-01, -2.7503e-01,  1.1143e-01, -5.6375e-01,\n",
       "          2.2442e-01, -2.4051e-01, -3.7259e-01,  2.4827e-01, -4.7673e-01,\n",
       "         -3.8789e-01, -3.1989e-01, -1.5469e-01,  1.0103e-01,  2.3085e-01]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c724425-8380-4eb3-8610-a8942e0cad6a",
   "metadata": {},
   "source": [
    "### The initial dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223bbf4b-0deb-4337-843a-d5a35452fed4",
   "metadata": {},
   "source": [
    "The weight initial values with the 4 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6b39896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[ 0.0831,  0.0224,  0.1531,  ...,  0.1413,  0.0540,  0.0741],\n",
       "                      [ 0.0662,  0.0010,  0.1485,  ...,  0.0335,  0.0900,  0.1510],\n",
       "                      [ 0.0128, -0.0897, -0.0168,  ...,  0.1347, -0.0138,  0.1366],\n",
       "                      ...,\n",
       "                      [ 0.0172,  0.0685,  0.1596,  ..., -0.0977, -0.1345,  0.1724],\n",
       "                      [ 0.0812, -0.0550,  0.0471,  ...,  0.1455,  0.0330,  0.1082],\n",
       "                      [-0.0687,  0.1496, -0.0260,  ..., -0.0008,  0.0710, -0.1402]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[-0.0877,  0.1369,  0.0201,  ..., -0.0538,  0.0559, -0.0571],\n",
       "                      [-0.0083, -0.0460, -0.0495,  ...,  0.1036, -0.0824, -0.0321],\n",
       "                      [ 0.0777,  0.0244,  0.0257,  ...,  0.1305, -0.0999, -0.1050],\n",
       "                      ...,\n",
       "                      [ 0.0337, -0.0537,  0.0896,  ..., -0.0816,  0.0030,  0.0837],\n",
       "                      [ 0.0438,  0.0359,  0.0821,  ...,  0.0264,  0.0589,  0.1025],\n",
       "                      [ 0.1107, -0.1158,  0.0321,  ..., -0.0845,  0.0030, -0.0828]]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3639ae",
   "metadata": {},
   "source": [
    "The three input matrices are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8435791e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()['in_proj_weight'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e7253",
   "metadata": {},
   "source": [
    "The output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b77b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()['out_proj.weight'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b4492",
   "metadata": {},
   "source": [
    "### By-passing the dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3cb2d",
   "metadata": {},
   "source": [
    "We create identity matrices to pass through the dense layers and recover the attention values and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64bd80d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_50 = torch.eye(50)\n",
    "i_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37832b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['out_proj.weight'][:] = i_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "169efcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()['in_proj_weight'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b863032a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_3_50 = torch.vstack((i_50, i_50, i_50))\n",
    "i_3_50.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea234af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['in_proj_weight'][:] = i_3_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b879275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 1.]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 1.]]))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f34906-38db-488d-b127-c76fc070e7b1",
   "metadata": {},
   "source": [
    "### Multihead attention without the dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b083c3-77b0-4115-a3aa-7041a9f8be15",
   "metadata": {},
   "source": [
    "We obtain now the same results as the `self_attention()` function for _ship:_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de86e9c-c982-4eb0-a725-595ee7bd9782",
   "metadata": {},
   "source": [
    "The attention scores for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f5c93cd-f8dd-4611-bee7-98eec7e69438",
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn_output, attn_scores) = att_layer(embeddings_seq_o, embeddings_seq_o, embeddings_seq_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926183c-0734-49c7-9480-190f6c892adf",
   "metadata": {},
   "source": [
    "The attention vector for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "197dc6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e0315",
   "metadata": {},
   "source": [
    "The embedding vector for _ship_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1be2d12f-9e76-4f53-baea-8178dd5a7e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0387,  0.1033,  0.3426, -0.4320,  0.2237, -0.0958, -0.9926,  0.6662,\n",
       "         0.4424, -0.7942,  0.5638,  0.9921,  0.0205,  0.5082,  0.0743, -0.1773,\n",
       "        -0.3408,  0.5674, -1.1545, -0.5718,  0.4288,  0.4191, -0.0658, -0.3339,\n",
       "         0.6682, -1.7473, -0.0485,  0.1531,  0.8642, -0.1447,  2.6571, -0.0545,\n",
       "        -0.5343,  0.3160,  0.4041,  0.2277,  0.3958, -0.2916, -0.1126, -0.1385,\n",
       "         0.1744, -0.5375,  0.9499, -0.4145, -0.1039, -0.1755, -0.2213, -0.3995,\n",
       "         0.2119, -0.3610], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad8d94-0a2d-4e16-ad54-151f2e9facad",
   "metadata": {},
   "source": [
    "## Test with a simple matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eda3f0-52b7-4c03-9167-7e331a018853",
   "metadata": {},
   "source": [
    "Three words, dimension of embeddings: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2663ebcb-8e4b-4720-9ed3-c88a950dc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_sequence = torch.tensor([[1.0, 0.0, 0.0, 1.0],\n",
    "                                 [0.0, 1.5, 1.0, 1.0],\n",
    "                                 [0.0, 1.0, 1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "946d88eb-7522-476b-80d4-ceac7af6a2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_sequence.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab40159-3c6a-45b7-87c4-3fa6f52186a3",
   "metadata": {},
   "source": [
    "### Self-attention from the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5bca6930-4deb-456b-8ac7-7bc92d77f51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4519, 0.6852, 0.5481, 1.0000],\n",
       "         [0.1045, 1.1609, 0.8955, 1.0000],\n",
       "         [0.1387, 1.1034, 0.8613, 1.0000]]),\n",
       " tensor([[0.4519, 0.2741, 0.2741],\n",
       "         [0.1045, 0.5307, 0.3648],\n",
       "         [0.1387, 0.4842, 0.3771]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention(test_input_sequence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1cdbf2f-dc0a-4354-bf96-30b99cddee20",
   "metadata": {},
   "source": [
    "### Multihead attention from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "527fc0db-4fd9-413e-9634-a54469adb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer = MultiheadAttention(4, \n",
    "                               1, \n",
    "                               bias=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0909144a-4c17-45de-82f1-3cb8547eb985",
   "metadata": {},
   "source": [
    "The multihead attention uses a Xavier initialization of the dense layers. The results will be different for those of `self_attention()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ffd9a3b5-4645-4fa5-8d84-06cc6dd72822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0728, -0.8113, -0.4689,  0.6076],\n",
       "         [-1.0398, -0.7560, -0.4240,  0.6074],\n",
       "         [-1.0482, -0.7704, -0.4357,  0.6072]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.2403, 0.4111, 0.3487],\n",
       "         [0.3710, 0.3092, 0.3198],\n",
       "         [0.3352, 0.3318, 0.3331]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer(test_input_sequence, \n",
    "          test_input_sequence,\n",
    "          test_input_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e889b-a828-40cc-b208-c6688be25c14",
   "metadata": {},
   "source": [
    "Weights of the dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b597eee-fee4-4a15-9aaf-d62e7e9c1b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[ 6.1394e-02, -6.1015e-01,  4.7498e-01,  1.9639e-01],\n",
       "                      [-5.7650e-01,  4.3781e-01,  4.9654e-03, -3.5933e-01],\n",
       "                      [-4.1028e-01, -4.8474e-02, -5.4061e-01,  5.4894e-01],\n",
       "                      [ 5.9218e-01,  4.1155e-01, -3.4587e-01,  2.5921e-01],\n",
       "                      [-5.4377e-04,  5.5142e-02,  4.4350e-01,  3.0742e-01],\n",
       "                      [-1.3947e-01, -5.4506e-01, -2.2638e-01,  5.4712e-01],\n",
       "                      [ 5.2317e-01,  2.6272e-01,  2.5883e-01, -3.7431e-01],\n",
       "                      [-4.4056e-01,  1.1487e-01, -5.2629e-01,  2.9210e-01],\n",
       "                      [-4.6415e-01, -3.9766e-01, -5.8648e-01, -6.0914e-01],\n",
       "                      [ 3.8215e-01,  9.8802e-02,  4.3574e-01,  2.6193e-01],\n",
       "                      [-4.5063e-01, -5.5486e-01, -4.9369e-01, -3.5490e-01],\n",
       "                      [-5.2275e-02,  6.0433e-01,  4.1626e-01, -8.4171e-02]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[ 0.4446, -0.1402,  0.3657,  0.2842],\n",
       "                      [ 0.1140,  0.0051,  0.4870,  0.0381],\n",
       "                      [ 0.1733,  0.3480,  0.3343, -0.0154],\n",
       "                      [-0.2058,  0.0882, -0.3598, -0.3420]]))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed433e0c-ebad-4319-926c-8d67af07af67",
   "metadata": {},
   "source": [
    "### By-passing the dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83941332-2d50-431f-ad4a-3e5e9b0722e7",
   "metadata": {},
   "source": [
    "We use weights of identity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad812cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_4 = torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ebcab2b-c158-41e9-9d5e-4aed62d74416",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['out_proj.weight'][:] = i_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8bbcedb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_3_4 = torch.vstack((i_4, i_4, i_4))\n",
    "i_3_4.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa12ec3-7044-4440-abdc-e6786dd078eb",
   "metadata": {},
   "source": [
    "We set these weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42b0789a-e4c6-4198-ab99-b74fbbc3d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['in_proj_weight'][:] = i_3_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c8facb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.],\n",
       "                      [1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.],\n",
       "                      [1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.]]))])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8892df4-4e1e-4c02-9d00-04be89fc41fd",
   "metadata": {},
   "source": [
    "Now we have the same results as with `self_attention()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "67d2d604-eb6e-43e4-b51a-2d645f5eb227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4519, 0.6852, 0.5481, 1.0000],\n",
       "         [0.1045, 1.1609, 0.8955, 1.0000],\n",
       "         [0.1387, 1.1034, 0.8613, 1.0000]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.4519, 0.2741, 0.2741],\n",
       "         [0.1045, 0.5307, 0.3648],\n",
       "         [0.1387, 0.4842, 0.3771]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer(test_input_sequence, \n",
    "          test_input_sequence,\n",
    "          test_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d037377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
