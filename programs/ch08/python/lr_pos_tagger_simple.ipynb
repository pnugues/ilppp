{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging with logistic regression\n",
    "\n",
    "Author: Pierre Nugues\n",
    "\n",
    "A simple POS tagger using a context of five words and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import linear_model\n",
    "import datasets\n",
    "from context_dictorizer import ContextDictorizer, evaluate\n",
    "from ch06.python.conll_dictorizer import CoNLLDictorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'EWT'  # 'EWT' or 'PTB' # The English Web Treebank or the Penn Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CORPUS == 'EWT':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = datasets.load_ud_en_ewt()\n",
    "else:\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = datasets.load_conll2009_pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'form',\n",
       " 'lemma',\n",
       " 'upos',\n",
       " 'xpos',\n",
       " 'feats',\n",
       " 'head',\n",
       " 'deprel',\n",
       " 'head',\n",
       " 'deps',\n",
       " 'misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# newdoc id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000\\n# sent_id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-0001\\n# newpar id = weblog-juancole.com_juancole_20051126063000_ENG_20051126_063000-p0001\\n# text = Al-Zaman : American forces killed Shaikh Abdullah al-Ani, the preacher at the mosque in the town of Qaim, near the Syrian border.\\n1\\tAl\\tAl\\tPROPN\\tNNP\\tNumber=Sing\\t0\\troot\\t0:root\\tSpaceAfter=No\\n2\\t-\\t-\\tPUNCT\\tHYPH\\t_\\t1\\tpunct\\t1:punct\\tSpaceAfter=No\\n3\\tZaman\\tZam'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictorizing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the corpus word in a dictionary, where the keys are the CoNLL-U columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1',\n",
       "  'form': 'Al',\n",
       "  'lemma': 'Al',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '0:root',\n",
       "  'deprel': 'root',\n",
       "  'deps': 'SpaceAfter=No'},\n",
       " {'id': '2',\n",
       "  'form': '-',\n",
       "  'lemma': '-',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': 'HYPH',\n",
       "  'feats': '_',\n",
       "  'head': '1:punct',\n",
       "  'deprel': 'punct',\n",
       "  'deps': 'SpaceAfter=No'},\n",
       " {'id': '3',\n",
       "  'form': 'Zaman',\n",
       "  'lemma': 'Zaman',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '1:flat',\n",
       "  'deprel': 'flat',\n",
       "  'deps': '_'},\n",
       " {'id': '4',\n",
       "  'form': ':',\n",
       "  'lemma': ':',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': ':',\n",
       "  'feats': '_',\n",
       "  'head': '1:punct',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '_'},\n",
       " {'id': '5',\n",
       "  'form': 'American',\n",
       "  'lemma': 'American',\n",
       "  'upos': 'ADJ',\n",
       "  'xpos': 'JJ',\n",
       "  'feats': 'Degree=Pos',\n",
       "  'head': '6:amod',\n",
       "  'deprel': 'amod',\n",
       "  'deps': '_'},\n",
       " {'id': '6',\n",
       "  'form': 'forces',\n",
       "  'lemma': 'force',\n",
       "  'upos': 'NOUN',\n",
       "  'xpos': 'NNS',\n",
       "  'feats': 'Number=Plur',\n",
       "  'head': '7:nsubj',\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '_'},\n",
       " {'id': '7',\n",
       "  'form': 'killed',\n",
       "  'lemma': 'kill',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBD',\n",
       "  'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin',\n",
       "  'head': '1:parataxis',\n",
       "  'deprel': 'parataxis',\n",
       "  'deps': '_'},\n",
       " {'id': '8',\n",
       "  'form': 'Shaikh',\n",
       "  'lemma': 'Shaikh',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '7:obj',\n",
       "  'deprel': 'obj',\n",
       "  'deps': '_'},\n",
       " {'id': '9',\n",
       "  'form': 'Abdullah',\n",
       "  'lemma': 'Abdullah',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '8:flat',\n",
       "  'deprel': 'flat',\n",
       "  'deps': '_'},\n",
       " {'id': '10',\n",
       "  'form': 'al',\n",
       "  'lemma': 'al',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '8:flat',\n",
       "  'deprel': 'flat',\n",
       "  'deps': 'SpaceAfter=No'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names)\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "train_dict[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features and we store them in a dictionary. This $y$ output is extracted from the `upos` column for the Universal dependencies or `pos` column for the Penn Treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer = ContextDictorizer(output='upos', w_size=2)\n",
    "context_dictorizer.fit(train_dict)\n",
    "# Feature and response extraction\n",
    "X_dict, y = context_dictorizer.transform(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the Penn Treebank, we print the features to check they match Table 8.1 in my book (second edition). Use the range: `range(48759, 48795)` then.\n",
    "We use the training step extraction with the dynamic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'form_0': '__bos__', 'form_1': '__bos__', 'form_2': 'al', 'form_3': '-', 'form_4': 'zaman'}\tPROPN\n",
      "{'form_0': '__bos__', 'form_1': 'al', 'form_2': '-', 'form_3': 'zaman', 'form_4': ':'}\tPUNCT\n",
      "{'form_0': 'al', 'form_1': '-', 'form_2': 'zaman', 'form_3': ':', 'form_4': 'american'}\tPROPN\n",
      "{'form_0': '-', 'form_1': 'zaman', 'form_2': ':', 'form_3': 'american', 'form_4': 'forces'}\tPUNCT\n",
      "{'form_0': 'zaman', 'form_1': ':', 'form_2': 'american', 'form_3': 'forces', 'form_4': 'killed'}\tADJ\n",
      "{'form_0': ':', 'form_1': 'american', 'form_2': 'forces', 'form_3': 'killed', 'form_4': 'shaikh'}\tNOUN\n",
      "{'form_0': 'american', 'form_1': 'forces', 'form_2': 'killed', 'form_3': 'shaikh', 'form_4': 'abdullah'}\tVERB\n",
      "{'form_0': 'forces', 'form_1': 'killed', 'form_2': 'shaikh', 'form_3': 'abdullah', 'form_4': 'al'}\tPROPN\n",
      "{'form_0': 'killed', 'form_1': 'shaikh', 'form_2': 'abdullah', 'form_3': 'al', 'form_4': '-'}\tPROPN\n",
      "{'form_0': 'shaikh', 'form_1': 'abdullah', 'form_2': 'al', 'form_3': '-', 'form_4': 'ani'}\tPROPN\n"
     ]
    }
   ],
   "source": [
    "#context_dictorizer.print_example(train_dict)\n",
    "for i in range(10):\n",
    "#for i in range(48759, 48795): # For the Penn Treebank\n",
    "    print(str(X_dict[i]) + '\\t' + y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the symbols into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vectorizer = DictVectorizer()\n",
    "X = dict_vectorizer.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierre/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the test corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first dictorize the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1',\n",
       "  'form': 'What',\n",
       "  'lemma': 'what',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'WP',\n",
       "  'feats': 'PronType=Int',\n",
       "  'head': '0:root',\n",
       "  'deprel': 'root',\n",
       "  'deps': '_'},\n",
       " {'id': '2',\n",
       "  'form': 'if',\n",
       "  'lemma': 'if',\n",
       "  'upos': 'SCONJ',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '4:mark',\n",
       "  'deprel': 'mark',\n",
       "  'deps': '_'},\n",
       " {'id': '3',\n",
       "  'form': 'Google',\n",
       "  'lemma': 'Google',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '4:nsubj',\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '_'},\n",
       " {'id': '4',\n",
       "  'form': 'Morphed',\n",
       "  'lemma': 'morph',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBD',\n",
       "  'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin',\n",
       "  'head': '1:advcl:if',\n",
       "  'deprel': 'advcl',\n",
       "  'deps': '_'},\n",
       " {'id': '5',\n",
       "  'form': 'Into',\n",
       "  'lemma': 'into',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '6:case',\n",
       "  'deprel': 'case',\n",
       "  'deps': '_'},\n",
       " {'id': '6',\n",
       "  'form': 'GoogleOS',\n",
       "  'lemma': 'GoogleOS',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '4:obl:into',\n",
       "  'deprel': 'obl',\n",
       "  'deps': 'SpaceAfter=No'},\n",
       " {'id': '7',\n",
       "  'form': '?',\n",
       "  'lemma': '?',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '.',\n",
       "  'feats': '_',\n",
       "  'head': '4:punct',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '_'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = conll_dict.transform(test_sentences)\n",
    "test_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, model,\n",
    "                     context_dictorizer,\n",
    "                     dict_vectorizer,\n",
    "                     ppos_key='ppos'):\n",
    "    \"\"\"\n",
    "    Prediction using the words (lexical values)\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_dict, y = context_dictorizer.transform([sentence],\n",
    "                                             training_step=False)\n",
    "    X = dict_vectorizer.transform(X_dict)\n",
    "    y_pred_vec = model.predict(X)\n",
    "    \n",
    "    # We add the predictions in the ppos column\n",
    "    for row, y_pred in zip(sentence, y_pred_vec):\n",
    "        row[ppos_key] = y_pred\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_dict:\n",
    "    sentence = predict_sentence(sentence,\n",
    "                                model,\n",
    "                                context_dictorizer,\n",
    "                                dict_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1',\n",
       "  'form': 'What',\n",
       "  'lemma': 'what',\n",
       "  'upos': 'PRON',\n",
       "  'xpos': 'WP',\n",
       "  'feats': 'PronType=Int',\n",
       "  'head': '0:root',\n",
       "  'deprel': 'root',\n",
       "  'deps': '_',\n",
       "  'ppos': 'PRON'},\n",
       " {'id': '2',\n",
       "  'form': 'if',\n",
       "  'lemma': 'if',\n",
       "  'upos': 'SCONJ',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '4:mark',\n",
       "  'deprel': 'mark',\n",
       "  'deps': '_',\n",
       "  'ppos': 'SCONJ'},\n",
       " {'id': '3',\n",
       "  'form': 'Google',\n",
       "  'lemma': 'Google',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '4:nsubj',\n",
       "  'deprel': 'nsubj',\n",
       "  'deps': '_',\n",
       "  'ppos': 'PROPN'},\n",
       " {'id': '4',\n",
       "  'form': 'Morphed',\n",
       "  'lemma': 'morph',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBD',\n",
       "  'feats': 'Mood=Ind|Tense=Past|VerbForm=Fin',\n",
       "  'head': '1:advcl:if',\n",
       "  'deprel': 'advcl',\n",
       "  'deps': '_',\n",
       "  'ppos': 'VERB'},\n",
       " {'id': '5',\n",
       "  'form': 'Into',\n",
       "  'lemma': 'into',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN',\n",
       "  'feats': '_',\n",
       "  'head': '6:case',\n",
       "  'deprel': 'case',\n",
       "  'deps': '_',\n",
       "  'ppos': 'ADP'},\n",
       " {'id': '6',\n",
       "  'form': 'GoogleOS',\n",
       "  'lemma': 'GoogleOS',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP',\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': '4:obl:into',\n",
       "  'deprel': 'obl',\n",
       "  'deps': 'SpaceAfter=No',\n",
       "  'ppos': 'NOUN'},\n",
       " {'id': '7',\n",
       "  'form': '?',\n",
       "  'lemma': '?',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '.',\n",
       "  'feats': '_',\n",
       "  'head': '4:punct',\n",
       "  'deprel': 'punct',\n",
       "  'deps': '_',\n",
       "  'ppos': 'PUNCT'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, lexical model: 0.9039874287959143\n"
     ]
    }
   ],
   "source": [
    "good, bad = evaluate(test_dict, 'upos', 'ppos')\n",
    "print('Accuracy, lexical model:', good / (good + bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model to sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['That round table might collapse .',\n",
    "             'That man can learn well .',\n",
    "             'This man can swim well .',\n",
    "             'The man can simwo .',\n",
    "             'That round table might collapsex .']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert them into CoNLL-like tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_conll(sentence):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a CoNLL dict\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    column_names = ['id', 'form']\n",
    "    sentence = list(enumerate(sentence.split(), start=1))\n",
    "    conll_cols = ''\n",
    "    for tuple in sentence:\n",
    "        conll_cols += str(tuple[0]) + '\\t' + tuple[1] + '\\n'\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names)\n",
    "    sent_dict = conll_dict.transform(conll_cols)\n",
    "    return sent_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we tag them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'round', 'table', 'might', 'collapse', '.']\n",
      "['PRON', 'ADJ', 'NOUN', 'AUX', 'VERB', 'PUNCT']\n",
      "['that', 'man', 'can', 'learn', 'well', '.']\n",
      "['DET', 'NOUN', 'AUX', 'VERB', 'INTJ', 'PUNCT']\n",
      "['this', 'man', 'can', 'swim', 'well', '.']\n",
      "['DET', 'NOUN', 'AUX', 'VERB', 'ADV', 'PUNCT']\n",
      "['the', 'man', 'can', 'simwo', '.']\n",
      "['DET', 'NOUN', 'AUX', 'VERB', 'PUNCT']\n",
      "['that', 'round', 'table', 'might', 'collapsex', '.']\n",
      "['PRON', 'ADJ', 'NOUN', 'AUX', 'VERB', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    sentence = sentence_to_conll(sentence.lower())\n",
    "    y_test_pred_cat = predict_sentence(sentence,\n",
    "                                       model,\n",
    "                                       context_dictorizer,\n",
    "                                       dict_vectorizer)\n",
    "    print([y['form'] for y in y_test_pred_cat])\n",
    "    print([y['ppos'] for y in y_test_pred_cat])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
