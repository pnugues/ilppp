{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple implementation of CBOW\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, Average\n",
    "import regex as re\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding size and context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "w_size = 2\n",
    "c_size = w_size * 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dickens'  # 'homer' dickens' 'selma' 'big'\n",
    "colab = False # On my machine or on colab\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    BASE_PATH = '/content/drive/My Drive/Colab Notebooks/'\n",
    "else:\n",
    "    BASE_PATH = '../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_corpus(path):\n",
    "    files = get_files(path, 'txt')\n",
    "    files = [path + file for file in files]\n",
    "    print(files)\n",
    "    text = ''\n",
    "    for file in files:\n",
    "        text += open(file).read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../corpus/Dickens/Hard Times.txt', '../../../corpus/Dickens/Oliver Twist.txt', '../../../corpus/Dickens/Great Expectations.txt', '../../../corpus/Dickens/The Old Curiosity Shop.txt', '../../../corpus/Dickens/A Tale of Two Cities.txt', '../../../corpus/Dickens/Dombey and Son.txt', '../../../corpus/Dickens/The Pickwick Papers.txt', '../../../corpus/Dickens/Bleak House.txt', '../../../corpus/Dickens/Our Mutual Friend.txt', '../../../corpus/Dickens/The Mystery of Edwin Drood.txt', '../../../corpus/Dickens/Nicholas Nickleby.txt', '../../../corpus/Dickens/David Copperfield.txt', '../../../corpus/Dickens/Little Dorrit.txt', '../../../corpus/Dickens/A Christmas Carol in Prose.txt']\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'homer':\n",
    "    #text = 'Sing, O goddess, the anger of Achilles son of Peleus'.lower()\n",
    "    text1 = open(BASE_PATH + 'corpus/iliad.mb.txt', encoding='utf-8').read().lower()\n",
    "    text2 = open(BASE_PATH + 'corpus/odyssey.mb.txt', encoding='utf-8').read().lower()\n",
    "    text = text1 + text2\n",
    "    test_words = ['he', 'she', 'ulysses', 'penelope', 'achaeans', 'trojans']\n",
    "if dataset == 'dickens':\n",
    "    path = BASE_PATH + 'corpus/Dickens/'\n",
    "    text = load_corpus(path)\n",
    "    test_words = ['he', 'she', 'paris', 'london', 'table', 'rare', 'monday', 'sunday', 'man', 'woman', 'king', 'queen', 'boy',\n",
    "                  'girl']\n",
    "elif dataset == 'selma':\n",
    "    path = BASE_PATH + 'corpus/Selma/'\n",
    "    text = load_corpus(path)\n",
    "    test_words = ['han', 'hon', 'att', 'bord', 'bordet', 'måndag', 'söndag', 'man', 'kvinna', 'kung', 'drottning',\n",
    "                  'pojke', 'flicka']\n",
    "elif dataset == 'big':\n",
    "    path = BASE_PATH + 'corpus/Dickens/'\n",
    "    text = load_corpus(path)\n",
    "    path = BASE_PATH + 'corpus/Norvig/'\n",
    "    text += load_corpus(path)\n",
    "    test_words = ['he', 'she', 'paris', 'london', 'table', 'rare', 'monday', 'sunday', 'man', 'woman', 'king', 'queen', 'boy',\n",
    "                  'girl']   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set all the text in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard', 'times', 'and', 'reprinted', 'pieces']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "words = re.findall('\\p{L}+', text)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaron',\n",
       " 'aback',\n",
       " 'abaft',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandons',\n",
       " 'abase']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = sorted(list(set(words)))\n",
    "unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35221"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(unique_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for (i, word) in enumerate(unique_words)}\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "#word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_words = []\n",
    "y_words = []\n",
    "for i in range(len(words) - c_size + 1):\n",
    "    X_words.append(words[i: i + w_size] + words[i + w_size + 1: i + 2 * w_size + 1])\n",
    "    y_words.append(words[i + w_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hard', 'times', 'reprinted', 'pieces'],\n",
       " ['times', 'and', 'pieces', 'by'],\n",
       " ['and', 'reprinted', 'by', 'charles'],\n",
       " ['reprinted', 'pieces', 'charles', 'dickens'],\n",
       " ['pieces', 'by', 'dickens', 'with'],\n",
       " ['by', 'charles', 'with', 'illustrations'],\n",
       " ['charles', 'dickens', 'illustrations', 'by'],\n",
       " ['dickens', 'with', 'by', 'marcus'],\n",
       " ['with', 'illustrations', 'marcus', 'stone'],\n",
       " ['illustrations', 'by', 'stone', 'maurice']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3355452, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([list(map(lambda x: word2idx.get(x), x)) for x in X_words])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'reprinted',\n",
       " 'pieces',\n",
       " 'by',\n",
       " 'charles',\n",
       " 'dickens',\n",
       " 'with',\n",
       " 'illustrations',\n",
       " 'by',\n",
       " 'marcus']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(lambda x: word2idx.get(x), y_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            3522100   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 35221)             3557321   \n",
      "=================================================================\n",
      "Total params: 7,079,421\n",
      "Trainable params: 7,079,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=2 * w_size),\n",
    "    Lambda(lambda x: backend.mean(x, axis=1)),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2950/2950 [==============================] - 781s 265ms/step - loss: 7.2465 - val_loss: 6.1144\n",
      "Epoch 2/4\n",
      "2950/2950 [==============================] - 799s 271ms/step - loss: 5.9232 - val_loss: 5.9360\n",
      "Epoch 3/4\n",
      "2950/2950 [==============================] - 718s 243ms/step - loss: 5.7326 - val_loss: 5.8727\n",
      "Epoch 4/4\n",
      "2950/2950 [==============================] - 708s 240ms/step - loss: 5.6434 - val_loss: 5.8375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd3fbe7b790>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=1024, epochs=4,  validation_split=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim_vecs(vector, U, nbr_words=10):\n",
    "    # Here cosine distance and not cosine\n",
    "    # distance between equal vectors: 0. max distance: 2\n",
    "    dist = [cosine(vector, U[i, :]) if np.any(U[i, :]) else 2\n",
    "            for i in range(U.shape[0])]\n",
    "    sorted_vectors = sorted(range(len(dist)), key=lambda k: dist[k])\n",
    "    return sorted_vectors[1:nbr_words + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ['she', 'it', 'nobody', 'they', 'nicholas', 'i', 'herbert', 'everybody', 'oliver', 'we']\n",
      "she ['he', 'nobody', 'they', 'florence', 'it', 'i', 'herbert', 'estella', 'edith', 'bella']\n",
      "paris ['england', 'london', 'france', 'print', 'india', 'yorkshire', 'newgate', 'yarmouth', 'dover', 'parliament']\n",
      "london ['paris', 'england', 'india', 'france', 'yorkshire', 'town', 'dover', 'greta', 'canterbury', 'yarmouth']\n",
      "table ['ground', 'wall', 'counter', 'pavement', 'road', 'hearth', 'box', 'staircase', 'grass', 'carpet']\n",
      "rare ['terrible', 'special', 'trifling', 'delicious', 'mighty', 'desperate', 'tender', 'sturdy', 'singular', 'sober']\n",
      "monday ['thursday', 'wednesday', 'sunday', 'tuesday', 'noon', 'betimes', 'saturday', 'tiptoe', 'christmas', 'horseback']\n",
      "sunday ['saturday', 'monday', 'summer', 'wednesday', 'winter', 'day', 'christmas', 'previous', 'stage', 'betimes']\n",
      "man ['gentleman', 'woman', 'lady', 'person', 'ooman', 'soldier', 'chap', 'un', 'dog', 'creature']\n",
      "woman ['man', 'lady', 'gentleman', 'creature', 'ooman', 'girl', 'un', 'fellow', 'chap', 'soldier']\n",
      "king ['pastry', 'bride', 'midshipman', 'ship', 'principal', 'cathedral', 'tailor', 'animal', 'auctioneer', 'clerk']\n",
      "queen ['porter', 'flower', 'string', 'diamond', 'base', 'pattern', 'band', 'paving', 'sheep', 'dish']\n",
      "boy ['girl', 'child', 'schoolmaster', 'magistrate', 'sexton', 'fellow', 'dwarf', 'jew', 'major', 'grinder']\n",
      "girl ['boy', 'child', 'woman', 'fellow', 'creature', 'creetur', 'lady', 'feller', 'widow', 'villain']\n"
     ]
    }
   ],
   "source": [
    "most_sim_words = {}\n",
    "for w in test_words:\n",
    "    most_sim_words[w] = most_sim_vecs(vectors[word2idx[w]], vectors)\n",
    "    most_sim_words[w] = list(map(idx2word.get, most_sim_words[w]))\n",
    "    print(w, most_sim_words[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
