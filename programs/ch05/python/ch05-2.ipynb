{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Sequences\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import regex as re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nils Holgerssons underbara resa genom Sverige\\nSelm'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = '../../corpus/Selma.txt'\n",
    "text = open(file_name).read().strip()\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = re.findall(r'\\p{L}+', text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        if word in frequency:\n",
    "            frequency[word] += 1\n",
    "        else:\n",
    "            frequency[word] = 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze Selma Lagerlöf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "och \t 37799\n",
      "att \t 28914\n",
      "han \t 22743\n",
      "det \t 22087\n",
      "i \t 17072\n",
      "som \t 16790\n",
      "hade \t 14955\n",
      "på \t 14634\n",
      "hon \t 14093\n",
      "en \t 13921\n",
      "inte \t 13826\n",
      "var \t 12852\n",
      "de \t 12599\n",
      "den \t 11773\n",
      "för \t 9811\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency = count_unigrams(words)\n",
    "for word in sorted(frequency.keys(), key=frequency.get, reverse=True)[:15]:\n",
    "    print(word, '\\t', frequency[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the counts to pairs of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(words):\n",
    "    bigrams = [tuple(words[idx:idx + 2])\n",
    "               for idx in range(len(words) - 1)]\n",
    "    frequencies = {}\n",
    "    for bigram in bigrams:\n",
    "        if bigram in frequencies:\n",
    "            frequencies[bigram] += 1\n",
    "        else:\n",
    "            frequencies[bigram] = 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('det', 'var') \t 4024\n",
      "('att', 'han') \t 3064\n",
      "('för', 'att') \t 3007\n",
      "('han', 'hade') \t 2352\n",
      "('att', 'det') \t 2152\n",
      "('det', 'är') \t 2114\n",
      "('att', 'hon') \t 1854\n",
      "('hon', 'hade') \t 1469\n",
      "('att', 'de') \t 1365\n",
      "('så', 'att') \t 1313\n",
      "('att', 'jag') \t 1187\n",
      "('han', 'var') \t 1061\n",
      "('han', 'inte') \t 1023\n",
      "('var', 'det') \t 1021\n",
      "('som', 'hade') \t 1002\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency_bigrams = count_bigrams(words)\n",
    "for bigram in sorted(frequency_bigrams.keys(), key=frequency_bigrams.get, reverse=True)[:15]:\n",
    "    print(bigram, '\\t', frequency_bigrams[bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trigrams(words):\n",
    "    trigrams = [tuple(words[idx:idx + 3])\n",
    "                for idx in range(len(words) - 2)]\n",
    "    frequencies = {}\n",
    "    for trigram in trigrams:\n",
    "        if trigram in frequencies:\n",
    "            frequencies[trigram] += 1\n",
    "        else:\n",
    "            frequencies[trigram] = 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('att', 'det', 'var') \t 555\n",
      "('att', 'han', 'hade') \t 422\n",
      "('att', 'han', 'inte') \t 410\n",
      "('det', 'var', 'en') \t 357\n",
      "('att', 'han', 'skulle') \t 333\n",
      "('det', 'var', 'inte') \t 298\n",
      "('att', 'hon', 'inte') \t 278\n",
      "('i', 'alla', 'fall') \t 272\n",
      "('men', 'det', 'var') \t 269\n",
      "('och', 'det', 'var') \t 253\n",
      "('så', 'att', 'han') \t 239\n",
      "('att', 'det', 'inte') \t 231\n",
      "('det', 'var', 'som') \t 231\n",
      "('som', 'han', 'hade') \t 226\n",
      "('att', 'hon', 'hade') \t 224\n",
      "('att', 'hon', 'skulle') \t 223\n",
      "('att', 'han', 'var') \t 222\n",
      "('för', 'att', 'få') \t 221\n",
      "('att', 'det', 'är') \t 220\n",
      "('som', 'om', 'han') \t 205\n",
      "('det', 'är', 'inte') \t 170\n",
      "('om', 'han', 'hade') \t 165\n",
      "('därför', 'att', 'han') \t 164\n",
      "('att', 'de', 'inte') \t 152\n",
      "('så', 'att', 'det') \t 151\n",
      "('för', 'att', 'se') \t 151\n",
      "('han', 'hade', 'varit') \t 151\n",
      "('än', 'en', 'gång') \t 149\n",
      "('att', 'de', 'hade') \t 145\n",
      "('att', 'jag', 'inte') \t 144\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency_trigrams = count_trigrams(words)\n",
    "for trigram in sorted(frequency_trigrams.keys(), key=frequency_trigrams.get, reverse=True)[:30]:\n",
    "    print(trigram, '\\t', frequency_trigrams[trigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries do not guarantee the order. We can sort according to the frequency and then the lexical order using a `lambda` function to define the sorting key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('att', 'det', 'var') \t 555\n",
      "('att', 'han', 'hade') \t 422\n",
      "('att', 'han', 'inte') \t 410\n",
      "('det', 'var', 'en') \t 357\n",
      "('att', 'han', 'skulle') \t 333\n",
      "('det', 'var', 'inte') \t 298\n",
      "('att', 'hon', 'inte') \t 278\n",
      "('i', 'alla', 'fall') \t 272\n",
      "('men', 'det', 'var') \t 269\n",
      "('och', 'det', 'var') \t 253\n",
      "('så', 'att', 'han') \t 239\n",
      "('att', 'det', 'inte') \t 231\n",
      "('det', 'var', 'som') \t 231\n",
      "('som', 'han', 'hade') \t 226\n",
      "('att', 'hon', 'hade') \t 224\n",
      "('att', 'hon', 'skulle') \t 223\n",
      "('att', 'han', 'var') \t 222\n",
      "('för', 'att', 'få') \t 221\n",
      "('att', 'det', 'är') \t 220\n",
      "('som', 'om', 'han') \t 205\n",
      "('det', 'är', 'inte') \t 170\n",
      "('om', 'han', 'hade') \t 165\n",
      "('därför', 'att', 'han') \t 164\n",
      "('att', 'de', 'inte') \t 152\n",
      "('för', 'att', 'se') \t 151\n",
      "('han', 'hade', 'varit') \t 151\n",
      "('så', 'att', 'det') \t 151\n",
      "('än', 'en', 'gång') \t 149\n",
      "('att', 'de', 'hade') \t 145\n",
      "('att', 'jag', 'inte') \t 144\n"
     ]
    }
   ],
   "source": [
    "for trigram in sorted(frequency_trigrams.keys(), key=lambda x: (-frequency_trigrams.get(x), x))[:30]:\n",
    "    print(trigram, '\\t', frequency_trigrams[trigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(words, n):\n",
    "    ngrams = [tuple(words[idx:idx + n])\n",
    "              for idx in range(len(words) - n + 1)]\n",
    "    # \"\\t\".join(words[idx:idx + n])\n",
    "    frequencies = {}\n",
    "    for ngram in ngrams:\n",
    "        if ngram in frequencies:\n",
    "            frequencies[ngram] += 1\n",
    "        else:\n",
    "            frequencies[ngram] = 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('haver', 'sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom') \t 6\n",
      "('japan', 'som', 'det', 'var', 'bom', 'bom', 'bom', 'å', 'rulla', 'bom') \t 6\n",
      "('metz', 'japan', 'som', 'det', 'var', 'bom', 'bom', 'bom', 'å', 'rulla') \t 6\n",
      "('portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom', 'bom', 'bom', 'å') \t 6\n",
      "('sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom', 'bom') \t 6\n",
      "('som', 'det', 'var', 'bom', 'bom', 'bom', 'å', 'rulla', 'bom', 'bom') \t 6\n",
      "('som', 'tidningen', 'haver', 'sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det') \t 6\n",
      "('tidningen', 'haver', 'sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var') \t 6\n",
      "('österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom', 'bom', 'bom') \t 6\n",
      "('han', 'satt', 'i', 'nästa', 'rum', 'och', 'så', 'kastade', 'han', 'sig') \t 5\n",
      "('i', 'nästa', 'rum', 'och', 'så', 'kastade', 'han', 'sig', 'över', 'oss') \t 5\n",
      "('satt', 'i', 'nästa', 'rum', 'och', 'så', 'kastade', 'han', 'sig', 'över') \t 5\n",
      "('att', 'säga', 'hur', 'de', 'togo', 'sig', 'ut', 'på', 'nära', 'håll') \t 4\n",
      "('bara', 'och', 'språkade', 'han', 'satt', 'i', 'nästa', 'rum', 'och', 'så') \t 4\n",
      "('folk', 'vill', 'lyda', 'mitt', 'råd', 'bör', 'det', 'genast', 'flytta', 'mot') \t 4\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency_ngrams = count_ngrams(words, N)\n",
    "for ngram in sorted(frequency_ngrams.keys(), key=lambda x: (-frequency_ngrams.get(x), x))[:15]:\n",
    "    print(ngram, '\\t', frequency_ngrams[ngram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooccurrence measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the computations, we need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = count_unigrams(words)\n",
    "frequency_bigrams = count_bigrams(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(words, freq_unigrams, freq_bigrams):\n",
    "    mi = {}\n",
    "    factor = len(words) * len(words) / (len(words) - 1)\n",
    "    for bigram in freq_bigrams:\n",
    "        mi[bigram] = (\n",
    "            math.log(factor * freq_bigrams[bigram] /\n",
    "                     (freq_unigrams[bigram[0]] *\n",
    "                      freq_unigrams[bigram[1]]), 2))\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info(words, frequency, frequency_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information is highly biased toward low-frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "filtered_mi = {k: v for k, v in mi.items() if frequency_bigrams[k] >= cutoff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('atterdag', 'brandskattar') \t 5 \t 7 \t 5 \t 17.068629848637798\n",
      "('el', 'aksa') \t 7 \t 5 \t 5 \t 17.068629848637798\n",
      "('metz', 'japan') \t 6 \t 7 \t 6 \t 17.068629848637798\n",
      "('new', 'york') \t 7 \t 7 \t 7 \t 17.068629848637798\n",
      "('rättar', 'söderlind') \t 7 \t 5 \t 5 \t 17.068629848637798\n",
      "('portugal', 'metz') \t 8 \t 6 \t 6 \t 16.8759847706954\n",
      "('xxii', 'karrs') \t 8 \t 8 \t 8 \t 16.8759847706954\n",
      "('österrike', 'portugal') \t 6 \t 8 \t 6 \t 16.8759847706954\n",
      "('valdemar', 'atterdag') \t 9 \t 5 \t 5 \t 16.70605976925309\n",
      "('xliii', 'västerbotten') \t 5 \t 9 \t 5 \t 16.70605976925309\n",
      "('neljä', 'viisi') \t 7 \t 7 \t 5 \t 16.583203021467558\n",
      "('svängda', 'brätten') \t 9 \t 6 \t 5 \t 16.443025363419295\n",
      "('tidiga', 'morgonstunden') \t 11 \t 5 \t 5 \t 16.416553152058103\n",
      "('sophie', 'elkan') \t 11 \t 11 \t 10 \t 16.279049628308172\n",
      "('britta', 'lambert') \t 13 \t 9 \t 9 \t 16.17554505255431\n"
     ]
    }
   ],
   "source": [
    "for bigram in sorted(filtered_mi.keys(), key=lambda x: (-filtered_mi.get(x), x))[:15]:\n",
    "    print(bigram, '\\t',\n",
    "          frequency[bigram[0]], '\\t',\n",
    "          frequency[bigram[1]], '\\t',\n",
    "          frequency_bigrams[bigram], '\\t',\n",
    "          filtered_mi[bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio(words, freq_unigrams, freq_bigrams):\n",
    "    lr = {}\n",
    "    for bigram in freq_bigrams:\n",
    "        p = freq_unigrams[bigram[1]] / len(words)\n",
    "        p1 = freq_bigrams[bigram] / freq_unigrams[bigram[0]]\n",
    "        p2 = ((freq_unigrams[bigram[1]] - freq_bigrams[bigram])\n",
    "              / (len(words) - freq_unigrams[bigram[0]]))\n",
    "        if p1 != 1.0 and p2 != 0.0:\n",
    "            lr[bigram] = 2.0 * (\n",
    "                log_f(freq_bigrams[bigram],\n",
    "                      freq_unigrams[bigram[0]], p1) +\n",
    "                log_f(freq_unigrams[bigram[1]] -\n",
    "                      freq_bigrams[bigram],\n",
    "                      len(words) - freq_unigrams[bigram[0]], p2) -\n",
    "                log_f(freq_bigrams[bigram],\n",
    "                      freq_unigrams[bigram[0]], p) -\n",
    "                log_f(freq_unigrams[bigram[1]] -\n",
    "                      freq_bigrams[bigram],\n",
    "                      len(words) - freq_unigrams[bigram[0]], p))\n",
    "    return lr\n",
    "\n",
    "\n",
    "def log_f(k, N, p):\n",
    "    return k * math.log(p) + (N - k) * math.log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('det', 'var') \t 22087 \t 12852 \t 4024 \t 15501.405961381693\n",
      "('för', 'att') \t 9811 \t 28914 \t 3007 \t 9674.805977699376\n",
      "('det', 'är') \t 22087 \t 6604 \t 2114 \t 8070.401027389045\n",
      "('ett', 'par') \t 5224 \t 800 \t 776 \t 8000.628446635721\n",
      "('han', 'hade') \t 22743 \t 14955 \t 2352 \t 5393.1480120694905\n",
      "('hade', 'varit') \t 14955 \t 1824 \t 889 \t 4957.154755381902\n",
      "('att', 'han') \t 28914 \t 22743 \t 3064 \t 4912.6058663810545\n",
      "('in', 'i') \t 2147 \t 17072 \t 856 \t 4101.641612320935\n",
      "('en', 'gång') \t 13921 \t 1332 \t 695 \t 4096.222880805115\n",
      "('därför', 'att') \t 906 \t 28914 \t 666 \t 3650.285979713226\n",
      "('klara', 'gulla') \t 304 \t 214 \t 213 \t 3627.243582541728\n",
      "('annat', 'än') \t 982 \t 2630 \t 407 \t 3540.9682107325316\n",
      "('sven', 'elversson') \t 346 \t 232 \t 215 \t 3464.0422768111766\n",
      "('hon', 'hade') \t 14093 \t 14955 \t 1469 \t 3318.3298605444725\n",
      "('hade', 'kommit') \t 14955 \t 923 \t 538 \t 3257.5113136380414\n"
     ]
    }
   ],
   "source": [
    "lr = likelihood_ratio(words, frequency, frequency_bigrams)\n",
    "\n",
    "for bigram in sorted(lr, key=lambda x: (-lr.get(x), x))[:15]:\n",
    "    print(bigram, \"\\t\", frequency[bigram[0]], \"\\t\", frequency[bigram[1]], \"\\t\",\n",
    "          frequency_bigrams[bigram], '\\t', lr[bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_scores(words, freq_unigrams, freq_bigrams):\n",
    "    ts = {}\n",
    "    for bigram in freq_bigrams:\n",
    "        ts[bigram] = ((freq_bigrams[bigram] -\n",
    "                      freq_unigrams[bigram[0]] *\n",
    "                      freq_unigrams[bigram[1]] /\n",
    "                      len(words)) /\n",
    "                      math.sqrt(freq_bigrams[bigram]))\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('det', 'var') \t 22087 \t 12852 \t 4024 \t 58.784381299609926\n",
      "('för', 'att') \t 9811 \t 28914 \t 3007 \t 49.459768903363944\n",
      "('att', 'han') \t 28914 \t 22743 \t 3064 \t 43.006894978233305\n",
      "('det', 'är') \t 22087 \t 6604 \t 2114 \t 42.68121626005159\n",
      "('han', 'hade') \t 22743 \t 14955 \t 2352 \t 41.20875153156267\n",
      "('att', 'hon') \t 28914 \t 14093 \t 1854 \t 33.22275512003115\n",
      "('hon', 'hade') \t 14093 \t 14955 \t 1469 \t 32.612593974488576\n",
      "('att', 'det') \t 28914 \t 22087 \t 2152 \t 32.082389179672\n",
      "('hade', 'varit') \t 14955 \t 1824 \t 889 \t 28.865294187312568\n",
      "('så', 'att') \t 9558 \t 28914 \t 1313 \t 28.308947395316558\n",
      "('in', 'i') \t 2147 \t 17072 \t 856 \t 27.95547341366061\n",
      "('ett', 'par') \t 5224 \t 800 \t 776 \t 27.700858955995376\n",
      "('att', 'de') \t 28914 \t 12599 \t 1365 \t 26.698575301782878\n",
      "('jag', 'har') \t 9640 \t 5138 \t 805 \t 26.558234722068747\n",
      "('att', 'jag') \t 28914 \t 9640 \t 1187 \t 26.044866545482744\n"
     ]
    }
   ],
   "source": [
    "ts = t_scores(words, frequency, frequency_bigrams)\n",
    "\n",
    "for bigram in sorted(ts, key=lambda x: (-ts.get(x), x))[:15]:\n",
    "    print(bigram, \"\\t\", frequency[bigram[0]], \"\\t\", frequency[bigram[1]], \"\\t\",\n",
    "          frequency_bigrams[bigram], '\\t', ts[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
