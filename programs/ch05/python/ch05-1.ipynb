{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import regex as re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization has no unique solution. Let us explore some possible strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us take a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Tell me, O muse, of that ingenious hero who\n",
    "travelled far and wide after he had sacked the famous\n",
    "town of Troy.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using boundaries: A first tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " 'O',\n",
       " 'muse',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"uses the nonletters to break the text into words\n",
    "    returns a list of words\"\"\"\n",
    "    # words = re.split(r'[\\s\\-,;:!?.’\\'«»()–...&‘’“”*—]+', text)\n",
    "    # words = re.split(r'[^a-zåàâäæçéèêëîïôöœßùûüÿA-ZÅÀÂÄÆÇÉÈÊËÎÏÔÖŒÙÛÜŸ’\\-]+', text)\n",
    "    # words = re.split(r'\\W+', text)\n",
    "    words = re.split(r'\\P{L}+', text)\n",
    "    words.remove('')\n",
    "    return words\n",
    "\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using content: A second one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " 'O',\n",
       " 'muse',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize2(text):\n",
    "    \"\"\"uses the letters to break the text into words\n",
    "    returns a list of words\"\"\"\n",
    "    # words = re.findall(r'[a-zåàâäæçéèêëîïôöœßùûüÿA-ZÅÀÂÄÆÇÉÈÊËÎÏÔÖŒÙÛÜŸ’\\-]+', text)\n",
    "    # words = re.findall(r'\\w+', text)\n",
    "    words = re.findall(r'\\p{L}+', text)\n",
    "    return words\n",
    "\n",
    "tokenize2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using punctuation: A third one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize3(text):\n",
    "    \"\"\"uses the punctuation and nonletters to break the text into words\n",
    "    returns a list of words\"\"\"\n",
    "    # text = re.sub(r'[^a-zåàâäæçéèêëîïôöœßùûüÿA-ZÅÀÂÄÆÇÉÈÊËÎÏÔÖŒÙÛÜŸ’'()\\-,.?!:;]+', '\\n', text)\n",
    "    # text = re.sub('([,.?!:;)('-])', r'\\n\\1\\n', text)\n",
    "    text = re.sub(r'[^\\p{L}\\p{P}]+', '\\n', text)\n",
    "    text = re.sub(r'(\\p{P})', r'\\n\\1\\n', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    return text.split()\n",
    "\n",
    "tokenize3(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tell',\n",
       " 'me',\n",
       " ',',\n",
       " 'O',\n",
       " 'muse',\n",
       " ',',\n",
       " 'of',\n",
       " 'that',\n",
       " 'ingenious',\n",
       " 'hero',\n",
       " 'who',\n",
       " 'travelled',\n",
       " 'far',\n",
       " 'and',\n",
       " 'wide',\n",
       " 'after',\n",
       " 'he',\n",
       " 'had',\n",
       " 'sacked',\n",
       " 'the',\n",
       " 'famous',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Troy',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize4(text):\n",
    "    \"\"\"uses the punctuation and symbols to break the text into words\n",
    "    returns a list of words\"\"\"\n",
    "    spaced_tokens = re.sub(r'([\\p{S}\\p{P}])', r' \\1 ', text)\n",
    "    # print(spaced_tokens)\n",
    "    one_token_per_line = re.sub(r'\\p{Z}+', '\\n', spaced_tokens)\n",
    "    # print(one_token_per_line)\n",
    "    tokens = one_token_per_line.split()\n",
    "    return tokens\n",
    "\n",
    "tokenize4(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nils Holgerssons underbara resa genom Sverige\\nSelma Lagerlöf\\n\\nInnehåll\\n\\tDen kristna dagvisan - Sveri'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = '../../corpus/Selma.txt'\n",
    "text = open(file_name).read().strip()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting and sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = re.findall(r'\\p{L}+', text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        if word in frequency:\n",
    "            frequency[word] += 1\n",
    "        else:\n",
    "            frequency[word] = 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze Selma Lagerlöf's novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "och \t 37799\n",
      "att \t 28914\n",
      "han \t 22743\n",
      "det \t 22087\n",
      "i \t 17072\n",
      "som \t 16790\n",
      "hade \t 14955\n",
      "på \t 14634\n",
      "hon \t 14093\n",
      "en \t 13921\n",
      "inte \t 13826\n",
      "var \t 12852\n",
      "de \t 12599\n",
      "den \t 11773\n",
      "för \t 9811\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency = count_unigrams(words)\n",
    "for word in sorted(frequency.keys(), key=frequency.get, reverse=True)[:15]:\n",
    "    print(word, '\\t', frequency[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
