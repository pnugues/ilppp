{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple implementation of skipgrams with negative sampling\n",
    "Author: Pierre Nugues\n",
    "\n",
    "Adapted from _Distributed Representations of Words and Phrases and their Compositionality_, Sect. 2.2, by Mikolov et al. 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, Average, GlobalAveragePooling1D, Dot, Input, Reshape, Activation\n",
    "import regex as re\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "from random import shuffle, randint\n",
    "from collections import Counter\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding size, context size, and negative counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "w_size = 2\n",
    "c_size = w_size * 2 + 1\n",
    "K_NEG = 5\n",
    "t = 1e-3\n",
    "power = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a dataset and execute locally or on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dickens'  # 'homer' dickens' 'selma' 'big'\n",
    "colab = False # On my machine or on colab\n",
    "debug = False\n",
    "DOWNSAMPLING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    BASE_PATH = '/content/drive/My Drive/Colab Notebooks/'\n",
    "else:\n",
    "    BASE_PATH = '../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_corpus(path):\n",
    "    files = get_files(path, 'txt')\n",
    "    files = [path + file for file in files]\n",
    "    print(files)\n",
    "    text = ''\n",
    "    for file in files:\n",
    "        text += open(file).read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../corpus/Dickens/Hard Times.txt', '../../../corpus/Dickens/Oliver Twist.txt', '../../../corpus/Dickens/Great Expectations.txt', '../../../corpus/Dickens/The Old Curiosity Shop.txt', '../../../corpus/Dickens/A Tale of Two Cities.txt', '../../../corpus/Dickens/Dombey and Son.txt', '../../../corpus/Dickens/The Pickwick Papers.txt', '../../../corpus/Dickens/Bleak House.txt', '../../../corpus/Dickens/Our Mutual Friend.txt', '../../../corpus/Dickens/The Mystery of Edwin Drood.txt', '../../../corpus/Dickens/Nicholas Nickleby.txt', '../../../corpus/Dickens/David Copperfield.txt', '../../../corpus/Dickens/Little Dorrit.txt', '../../../corpus/Dickens/A Christmas Carol in Prose.txt']\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'homer':\n",
    "    #text = 'Sing, O goddess, the anger of Achilles son of Peleus'.lower()\n",
    "    text1 = open(BASE_PATH + 'corpus/iliad.mb.txt', encoding='utf-8').read().lower()\n",
    "    text2 = open(BASE_PATH + 'corpus/odyssey.mb.txt', encoding='utf-8').read().lower()\n",
    "    text = text1 + text2\n",
    "    test_words = ['he', 'she', 'ulysses', 'penelope', 'achaeans', 'trojans']\n",
    "if dataset == 'dickens':\n",
    "    path = BASE_PATH + 'corpus/Dickens/'\n",
    "    text = load_corpus(path)\n",
    "    test_words = ['he', 'she', 'paris', 'london', 'table', 'rare', 'monday', 'sunday', 'man', 'woman', 'king', 'queen', 'boy',\n",
    "                  'girl']\n",
    "elif dataset == 'selma':\n",
    "    path = BASE_PATH + 'corpus/Selma/'\n",
    "    text = load_corpus(path)\n",
    "    test_words = ['han', 'hon', 'att', 'bord', 'bordet', 'måndag', 'söndag', 'man', 'kvinna', 'kung', 'drottning',\n",
    "                  'pojke', 'flicka']\n",
    "elif dataset == 'big':\n",
    "    path = BASE_PATH + 'corpus/Dickens/'\n",
    "    text = load_corpus(path)\n",
    "    path = BASE_PATH + 'corpus/Norvig/'\n",
    "    text += load_corpus(path)\n",
    "    test_words = ['he', 'she', 'paris', 'london', 'table', 'rare', 'monday', 'sunday', 'man', 'woman', 'king', 'queen', 'boy',\n",
    "                  'girl']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set all the text in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard', 'times', 'and', 'reprinted', 'pieces']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "word_seq = re.findall('\\p{L}+', text)\n",
    "word_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can downsample the frequent words. We first count the words. We will have to count them again after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157339"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['penelope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_probs = dict(counts)\n",
    "word_cnt = sum(discard_probs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in discard_probs:\n",
    "    discard_probs[key] = max(0, 1 - math.sqrt(t/(discard_probs[key]/word_cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard_probs['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard_probs['he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_word_seq = []\n",
    "for word in word_seq:\n",
    "    if discard_probs[word] < np.random.random():\n",
    "        subsampled_word_seq += [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNSAMPLING:\n",
    "    word_seq = subsampled_word_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157339"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['penelope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3355456"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnt = sum(counts.values())\n",
    "word_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aaron',\n",
       " 'aback',\n",
       " 'abaft',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandons',\n",
       " 'abase']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = sorted(list(counts.keys()))\n",
    "unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35221"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(unique_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for (i, word) in enumerate(unique_words)}\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "#word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the words to their indices and we get the sequence of word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14222, 31182, 1071, 25401, 22543]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widx_seq = list(map(word2idx.get, word_seq))\n",
    "widx_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a power tranform to a list of counts and we return power transformed probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform(counts, power):\n",
    "    trfmd_probs = dict()\n",
    "    for word in counts:\n",
    "        trfmd_probs[word] = math.pow(counts[word], power)\n",
    "    sum_probs = sum(trfmd_probs.values())\n",
    "    for word in trfmd_probs:\n",
    "        trfmd_probs[word] /= sum_probs\n",
    "    return trfmd_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfmd_probs = power_transform(counts, power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the index and proability lists for the random choice function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfmd_probs_idx = {word2idx[k]: v for k, v in trfmd_probs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_idx, probs = zip(*trfmd_probs_idx.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25126,\n",
       " 16334,\n",
       " 20392,\n",
       " 21249,\n",
       " 25901,\n",
       " 6318,\n",
       " 34385,\n",
       " 34357,\n",
       " 34385,\n",
       " 4,\n",
       " 22204,\n",
       " 29397,\n",
       " 26336,\n",
       " 17873,\n",
       " 21106,\n",
       " 20911,\n",
       " 20895,\n",
       " 3703,\n",
       " 903,\n",
       " 1224]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(draw_idx, weights=probs, k=K_NEG * 2 * w_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the words, we form positive and negative pairs. We extract the context words of a word from its neighbors in the word sequence to form the positive pairs and at random to form the negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_generator(widx_seq):\n",
    "    # A batch consists of the positive pairs generated by a word and its context\n",
    "    # and the negative pairs: w_size * 2 + K_NEG * w_size * 2 = (K_NEG + 1) * w_size * 2\n",
    "    for idx, widx in tqdm(enumerate(widx_seq[w_size:-w_size], w_size)):\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "        # We create the start and end indices as in range(start, end)\n",
    "        start_idx = idx - w_size\n",
    "        end_idx = idx + w_size + 1\n",
    "        # We create pairs from the left context: start_idx -> idx and from the right context idx + 1 -> end_idx\n",
    "        X_i = [widx_seq[idx]] * (K_NEG + 1) * 2 * w_size\n",
    "        X_c = [widx_seq[c_idx] for c_idx in [*range(start_idx, idx), *range(idx + 1, end_idx)]]\n",
    "        X_c += random.choices(draw_idx, weights=probs, k=K_NEG * 2 * w_size)\n",
    "        y = [1] * w_size * 2 + [0] * w_size * 2 * K_NEG\n",
    "        y = np.array(y)\n",
    "        X_i = np.array(X_i)\n",
    "        X_c = np.array(X_c)\n",
    "        yield X_i, X_c, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def minibatch_generator(widx_seq):\\n    # A batch consists of the positive pairs generated by a word and its context\\n    # and the negative pairs: w_size * 2 + K_NEG * w_size * 2 = (K_NEG + 1) * w_size * 2\\n    for idx, widx in tqdm(enumerate(widx_seq[w_size:-w_size], w_size)):\\n        positive_pairs = []\\n        negative_pairs = []\\n        # We create the start and end indices as in range(start, end)\\n        start_idx = idx - w_size\\n        end_idx = idx + w_size + 1\\n        # We create pairs from the left context: start_idx -> idx and from the right context idx + 1 -> end_idx\\n        X_i = [widx_seq[idx]] * (K_NEG + 1) * 2 * w_size\\n        X_c = [widx_seq[c_idx] for c_idx in [*range(start_idx, idx), *range(idx + 1, end_idx)]]\\n        X_c += [widx_seq[randint(0, len(widx_seq) - 1)] for _ in range(K_NEG * 2 * w_size)]\\n        y = [1] * w_size * 2 + [0] * w_size * 2 * K_NEG\\n        y = np.array(y)\\n        X_i = np.array(X_i)\\n        X_c = np.array(X_c)\\n        yield X_i, X_c, y'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old version\n",
    "\"\"\"def minibatch_generator(widx_seq):\n",
    "    # A batch consists of the positive pairs generated by a word and its context\n",
    "    # and the negative pairs: w_size * 2 + K_NEG * w_size * 2 = (K_NEG + 1) * w_size * 2\n",
    "    for idx, widx in tqdm(enumerate(widx_seq[w_size:-w_size], w_size)):\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "        # We create the start and end indices as in range(start, end)\n",
    "        start_idx = idx - w_size\n",
    "        end_idx = idx + w_size + 1\n",
    "        # We create pairs from the left context: start_idx -> idx and from the right context idx + 1 -> end_idx\n",
    "        X_i = [widx_seq[idx]] * (K_NEG + 1) * 2 * w_size\n",
    "        X_c = [widx_seq[c_idx] for c_idx in [*range(start_idx, idx), *range(idx + 1, end_idx)]]\n",
    "        X_c += [widx_seq[randint(0, len(widx_seq) - 1)] for _ in range(K_NEG * 2 * w_size)]\n",
    "        y = [1] * w_size * 2 + [0] * w_size * 2 * K_NEG\n",
    "        y = np.array(y)\n",
    "        X_i = np.array(X_i)\n",
    "        X_c = np.array(X_c)\n",
    "        yield X_i, X_c, y\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def minibatch_generator(widx_seq):\\n    # A batch consists of the positive pairs generated by a word and its context\\n    # and the negative pairs: w_size * 2 + K_NEG * w_size * 2 = (K_NEG + 1) * w_size * 2\\n    for idx, widx in tqdm(enumerate(widx_seq)):\\n        positive_pairs = []\\n        negative_pairs = []\\n        # We create the start and end indices as in range(start, end)\\n        start_idx = max(0, idx - w_size)\\n        end_idx = min(idx + w_size + 1, len(widx_seq))\\n        # We create pairs from the left context: start_idx -> idx and from the right context idx + 1 -> end_idx\\n        for c_idx in [*range(start_idx, idx), *range(idx + 1, end_idx)]:\\n            positive_pairs += [(widx_seq[idx], widx_seq[c_idx])]\\n            negative_pairs += [(widx_seq[idx], widx_seq[randint(0, len(widx_seq) - 1)]) for _ in range(K_NEG)]\\n        pairs = positive_pairs + negative_pairs\\n        X = np.array(pairs)\\n        X_i = X[:, 0]\\n        X_c = X[:, 1] \\n        y = [1] * len(positive_pairs) + [0] * len(negative_pairs)\\n        y = np.array(y)\\n        yield X_i, X_c, y'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old old version\n",
    "\"\"\"def minibatch_generator(widx_seq):\n",
    "    # A batch consists of the positive pairs generated by a word and its context\n",
    "    # and the negative pairs: w_size * 2 + K_NEG * w_size * 2 = (K_NEG + 1) * w_size * 2\n",
    "    for idx, widx in tqdm(enumerate(widx_seq)):\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "        # We create the start and end indices as in range(start, end)\n",
    "        start_idx = max(0, idx - w_size)\n",
    "        end_idx = min(idx + w_size + 1, len(widx_seq))\n",
    "        # We create pairs from the left context: start_idx -> idx and from the right context idx + 1 -> end_idx\n",
    "        for c_idx in [*range(start_idx, idx), *range(idx + 1, end_idx)]:\n",
    "            positive_pairs += [(widx_seq[idx], widx_seq[c_idx])]\n",
    "            negative_pairs += [(widx_seq[idx], widx_seq[randint(0, len(widx_seq) - 1)]) for _ in range(K_NEG)]\n",
    "        pairs = positive_pairs + negative_pairs\n",
    "        X = np.array(pairs)\n",
    "        X_i = X[:, 0]\n",
    "        X_c = X[:, 1] \n",
    "        y = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n",
    "        y = np.array(y)\n",
    "        yield X_i, X_c, y\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build two inputs: The left input is the input word and the right one is a context word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       3522100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       3522100     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 1)         2           dot[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 7,044,202\n",
      "Trainable params: 7,044,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i_word = Input(shape=(1,))\n",
    "i_embedding = Embedding(vocab_size, embedding_dim, input_length=1)(i_word)\n",
    "\n",
    "c_word = Input(shape=(1,))\n",
    "c_embedding = Embedding(vocab_size, embedding_dim, input_length=1)(c_word)\n",
    "\n",
    "dot_prod = Dot(axes=-1, normalize=True)([i_embedding, c_embedding])\n",
    "output = Dense(1, activation='sigmoid')(dot_prod)\n",
    "model = Model([i_word, c_word], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to measure the vector similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim_vecs(vector, U, nbr_words=10):\n",
    "    # Here cosine distance and not cosine\n",
    "    # distance between equal vectors: 0. max distance: 2\n",
    "    dist = [cosine(vector, U[i, :]) if np.any(U[i, :]) else 2\n",
    "            for i in range(U.shape[0])]\n",
    "    sorted_vectors = sorted(range(len(dist)), key=lambda k: dist[k])\n",
    "    return sorted_vectors[1:nbr_words + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_neg(y_true, y_hat):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    log_y_hat_1 = tf.math.log(y_hat)\n",
    "    log_y_hat_0 = tf.math.log(1.0 - y_hat)\n",
    "    loss = tf.math.add(tf.math.multiply(y_true, log_y_hat_1),\n",
    "                       tf.math.multiply(1.0 - y_true, log_y_hat_0))\n",
    "    loss = -tf.reduce_sum(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss_neg, optimizer='rmsprop')#, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369373it [2:01:59, 50.43it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, EPOCHS))\n",
    "    for i, batch in enumerate(minibatch_generator(widx_seq)):\n",
    "        X_i, X_c, y = batch\n",
    "        loss = model.train_on_batch([X_i, X_c], y)\n",
    "    print('Loss:', loss)\n",
    "    vectors = model.get_weights()[0]\n",
    "    most_sim_words = {}\n",
    "    for w in test_words:\n",
    "        most_sim_words[w] = most_sim_vecs(vectors[word2idx[w]], vectors)\n",
    "        most_sim_words[w] = list(map(idx2word.get, most_sim_words[w]))\n",
    "        print(w, most_sim_words[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss: 0.4462618827819824\n",
    "he ['she', 'they', 'it', 'ulysses', 'amopaon', 'i', 'bringing', 'lifting', 'achilles', 'manage']\n",
    "she ['he', 'ulysses', 'minerva', 'they', 'i', 'telemachus', 'achilles', 'bringing', 'vulcan', 'glad']\n",
    "ulysses ['telemachus', 'penelope', 'achilles', 'menelaus', 'she', 'minerva', 'agamemnon', 'hector', 'jove', 'apollo']\n",
    "penelope ['telemachus', 'ulysses', 'dear', 'jove', 'father', 'husband', 'mother', 'menelaus', 'wife', 'unhappy']\n",
    "achaeans ['trojans', 'others', 'gods', 'danaans', 'outer', 'suitors', 'argives', 'gates', 'other', 'ships']\n",
    "\n",
    "12it [00:00, 113.77it/s]\n",
    "\n",
    "trojans ['suitors', 'achaeans', 'sea', 'others', 'winds', 'battlements', 'corpses', 'ships', 'night', 'argives']\n",
    "Epoch 2/10\n",
    "\n",
    "272712it [39:46, 114.27it/s]\n",
    "\n",
    "Loss: 0.4378413259983063\n",
    "he ['she', 'they', 'achilles', 'it', 'i', 'somehow', 'mouthed', 'healed', 'gortyn', 'we']\n",
    "she ['he', 'minerva', 'they', 'telemachus', 'ulysses', 'i', 'sun', 'sidon', 'achilles', 'eumaeus']\n",
    "ulysses ['telemachus', 'penelope', 'menelaus', 'achilles', 'agamemnon', 'minerva', 'ajax', 'hector', 'neptune', 'alcinous']\n",
    "penelope ['telemachus', 'eumaeus', 'euryclea', 'ulysses', 'father', 'dear', 'mother', 'juno', 'unhappy', 'alcinous']\n",
    "achaeans ['trojans', 'gods', 'others', 'suitors', 'danaans', 'phaeacians', 'we', 'argives', 'corpses', 'dogs']\n",
    "\n",
    "13it [00:00, 117.94it/s]\n",
    "\n",
    "trojans ['achaeans', 'others', 'suitors', 'argives', 'themselves', 'ships', 'sea', 'corpses', 'night', 'battlements']\n",
    "Epoch 3/10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss: 9.454566955566406\n",
    "10 iter.\n",
    "he ['they', 'she', 'minerva', 'adraste', 'fee', 'parleying', 'i', 'harden', 'have', 'seat']\n",
    "she ['they', 'he', 'alcyone', 'minerva', 'helen', 'fickle', 'juno', 'sunium', 'heaven', 'penelope']\n",
    "ulysses ['telemachus', 'penelope', 'antinous', 'menelaus', 'aegisthus', 'mentor', 'eurymachus', 'alcinous', 'theoclymenus', 'mercury']\n",
    "penelope ['nausicaa', 'telemachus', 'madam', 'antinous', 'father', 'euryclea', 'mercury', 'menelaus', 'noemon', 'mentor']\n",
    "achaeans ['trojans', 'argives', 'suitors', 'gods', 'danaans', 'ships', 'sea', 'others', 'other', 'ground']\n",
    "trojans ['achaeans', 'argives', 'suitors', 'gods', 'ships', 'ground', 'sea', 'danaans', 'others', 'wall']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neg_loss and 3 epochs\n",
    "he ['she', 'corrections', 'quainter', 'tortuously', 'they', 'disgusts', 'legislature', 'decorously', 'applicants', 'topple']\n",
    "she ['he', 'florence', 'they', 'wastin', 'underhanded', 'everybody', 'powerfully', 'estella', 'bella', 'nobody']\n",
    "paris ['france', 'england', 'requisition', 'despair', 'italy', 'amazement', 'parliament', 'dust', 'convulsions', 'lincolnshire']\n",
    "london ['england', 'town', 'italy', 'france', 'vain', 'paris', 'lincolnshire', 'buckingham', 'itself', 'due']\n",
    "table ['window', 'fire', 'ground', 'wall', 'floor', 'sofa', 'box', 'road', 'carriage', 'desk']\n",
    "rare ['female', 'singular', 'monstrous', 'special', 'mere', 'moral', 'terrible', 'genteel', 'common', 'remarkable']\n",
    "monday ['wednesday', 'horseback', 'floor', 'thursday', 'board', 'tiptoe', 'sides', 'entering', 'saturday', 'sunday']\n",
    "sunday ['saturday', 'summer', 'post', 'coffee', 'board', 'stone', 'monday', 'wall', 'green', 'garden']\n",
    "man ['gentleman', 'woman', 'lady', 'person', 'boy', 'child', 'girl', 'dog', 'fellow', 'creature']\n",
    "woman ['lady', 'girl', 'gentleman', 'creature', 'man', 'fellow', 'person', 'boy', 'child', 'servant']\n",
    "king ['clergyman', 'maker', 'baker', 'cook', 'pawnbroker', 'chandler', 'attorney', 'bride', 'knights', 'mayor']\n",
    "aqueen ['forest', 'tumult', 'pile', 'roast', 'professor', 'soot', 'combination', 'mixture', 'bunch', 'depression']\n",
    "boy ['girl', 'child', 'lady', 'servant', 'woman', 'gentleman', 'doctor', 'baby', 'house', 'fellow']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neg_loss and 4 epochs, downsampling\n",
    "Loss: 9.66569995880127\n",
    "he ['cunningly', 'nigh', 'quick', 'trachis', 'despairingly', 'lesson', 'withdrew', 'she', 'keenly', 'bewail']\n",
    "she ['attempt', 'overtakes', 'aloud', 'piloted', 'gush', 'hawk', 'he', 'nausicaa', 'meliboea', 'thrash']\n",
    "ulysses ['swineherd', 'tried', 'indisputable', 'happen', 'invincible', 'arrive', 'accompanied', 'stains', 'cyclopes', 'rouse']\n",
    "penelope ['nurse', 'desire', 'telemachus', 'madam', 'stockman', 'angrily', 'reminds', 'eurynome', 'troubled', 'alas']\n",
    "achaeans ['danaans', 'argives', 'oracles', 'faint', 'pick', 'avail', 'knowledge', 'useful', 'people', 'unharnessed']\n",
    "trojans ['wander', 'perilous', 'erembians', 'maniac', 'evils', 'undaunted', 'argives', 'thwart', 'violate', 'warding']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "neg_loss and 4 epochs POW = 0.75?\n",
    "he ['she', 'they', 'achilles', 'i', 'bride', 'halt', 'ruined', 'mars', 'solitary', 'muttering']\n",
    "she ['minerva', 'he', 'they', 'handmaids', 'helen', 'heaven', 'maids', 'venus', 'wand', 'steps']\n",
    "ulysses ['telemachus', 'eumaeus', 'aegisthus', 'alcinous', 'antinous', 'mercury', 'penelope', 'menelaus', 'nestor', 'eurymachus']\n",
    "penelope ['antinous', 'eumaeus', 'telemachus', 'nurse', 'dear', 'alcinous', 'piteously', 'euryclea', 'mercury', 'leto']\n",
    "achaeans ['danaans', 'trojans', 'argives', 'gods', 'others', 'suitors', 'phaeacians', 'sea', 'alone', 'closely']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'gods', 'others', 'sea', 'earth', 'hindered', 'themselves', 'suitors']\n",
    "\n",
    "Loss: 9.882923126220703 POW = 1\n",
    "he ['she', 'they', 'key', 'i', 'we', 'destroys', 'achilles', 'glares', 'dearest', 'mercury']\n",
    "she ['he', 'minerva', 'they', 'penelope', 'melanthius', 'apollo', 'mercury', 'i', 'telemachus', 'alcyone']\n",
    "ulysses ['telemachus', 'antinous', 'achilles', 'penelope', 'agamemnon', 'eumaeus', 'minerva', 'piteously', 'euryclea', 'ajax']\n",
    "penelope ['telemachus', 'nurse', 'antinous', 'eumaeus', 'euryclea', 'juno', 'dear', 'apollo', 'thetis', 'menelaus']\n",
    "achaeans ['trojans', 'danaans', 'argives', 'others', 'suitors', 'gods', 'phaeacians', 'myrmidons', 'immortals', 'lycians']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'others', 'suitors', 'gods', 'lycians', 'myrmidons', 'themselves', 'driven']\n",
    "\n",
    "Loss: 9.43042278289795 POW = 1\n",
    "he ['they', 'she', 'healed', 'dagger', 'i', 'prisoner', 'we', 'apollo', 'horsehair', 'craftiest']\n",
    "she ['minerva', 'he', 'juno', 'they', 'venus', 'courtesy', 'maids', 'veil', 'penelope', 'iris']\n",
    "ulysses ['telemachus', 'agamemnon', 'achilles', 'eumaeus', 'neptune', 'nestor', 'menelaus', 'alcinous', 'idomeneus', 'penelope']\n",
    "penelope ['eumaeus', 'nurse', 'antinous', 'telemachus', 'euryclea', 'juno', 'neptune', 'alcinous', 'eurymachus', 'mother']\n",
    "achaeans ['trojans', 'argives', 'danaans', 'suitors', 'gods', 'others', 'phaeacians', 'themselves', 'fall', 'nothing']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'dogs', 'themselves', 'others', 'suitors', 'fall', 'sooner', 'gods']\n",
    "\n",
    "Loss: 11.508221626281738 POW = 0.75\n",
    "he ['they', 'she', 'offal', 'gorgon', 'place', 'undisturbed', 'prizes', 'hind', 'seat', 'steaks']\n",
    "she ['minerva', 'juno', 'vixen', 'arete', 'heron', 'he', 'they', 'beguiled', 'venus', 'moody']\n",
    "ulysses ['mercury', 'telemachus', 'penelope', 'antinous', 'leto', 'alcinous', 'swineherd', 'idomeneus', 'eumaeus', 'diomed']\n",
    "penelope ['euryclea', 'eumaeus', 'antinous', 'nurse', 'leto', 'alcinous', 'mentor', 'telemachus', 'dear', 'juno']\n",
    "achaeans ['argives', 'trojans', 'danaans', 'gods', 'suitors', 'others', 'ships', 'fighting', 'day', 'battle']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'ships', 'themselves', 'lycians', 'ground', 'sea', 'heroes', 'dogs']\n",
    "Loss: 9.757258415222168 POW = 0.75\n",
    "he ['they', 'she', 'it', 'dipping', 'targets', 'journey', 'devil', 'sunium', 'safely', 'democoon']\n",
    "she ['they', 'he', 'minerva', 'simple', 'juno', 'maids', 'slipped', 'calypso', 'husband', 'overboard']\n",
    "ulysses ['penelope', 'telemachus', 'antinous', 'menelaus', 'glad', 'alcinous', 'eumaeus', 'euryclea', 'eurymachus', 'nestor']\n",
    "penelope ['telemachus', 'antinous', 'eumaeus', 'alcinous', 'nurse', 'euryclea', 'ulysses', 'thetis', 'mercury', 'dear']\n",
    "achaeans ['danaans', 'trojans', 'argives', 'suitors', 'gods', 'others', 'immortals', 'sea', 'ships', 'myrmidons']\n",
    "trojans ['achaeans', 'danaans', 'argives', 'ships', 'suitors', 'earth', 'fighting', 'sea', 'themselves', 'immortals']\n",
    "\n",
    "Loss: 7.38845682144165 POW = 0.75 dot norm\n",
    "he ['she', 'they', 'minerva', 'scoundrel', 'i', 'achilles', 'we', 'apollo', 'ineffable', 'exposed']\n",
    "she ['he', 'minerva', 'juno', 'sun', 'they', 'phemius', 'venus', 'euryalus', 'penelope', 'apollo']\n",
    "ulysses ['achilles', 'telemachus', 'antinous', 'diomed', 'swineherd', 'antilochus', 'penelope', 'agamemnon', 'hector', 'menelaus']\n",
    "penelope ['euryclea', 'telemachus', 'dear', 'nurse', 'alcinous', 'answered', 'said', 'queen', 'o', 'eumaeus']\n",
    "achaeans ['trojans', 'danaans', 'argives', 'gods', 'suitors', 'others', 'immortals', 'sun', 'phaeacians', 'lycians']\n",
    "trojans ['achaeans', 'danaans', 'argives', 'others', 'lycians', 'dogs', 'suitors', 'immortals', 'gods', 'main']\n",
    "\n",
    "Loss: 9.18224811553955 POW = 0.75 dot norm\n",
    "he ['she', 'they', 'i', 'aim', 'penelope', 'glad', 'antinous', 'minerva', 'sun', 'adrestus']\n",
    "she ['he', 'minerva', 'juno', 'sarpedon', 'sun', 'they', 'venus', 'antinous', 'obeyed', 'maids']\n",
    "ulysses ['antinous', 'swineherd', 'ajax', 'menelaus', 'diomed', 'eurymachus', 'agamemnon', 'alcinous', 'eumaeus', 'achilles']\n",
    "penelope ['dear', 'father', 'eumaeus', 'telemachus', 'euryclea', 'nurse', 'piteously', 'mother', 'neighbour', 'antinous']\n",
    "achaeans ['danaans', 'trojans', 'gods', 'argives', 'others', 'phaeacians', 'immortals', 'suitors', 'we', 'birds']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'suitors', 'phaeacians', 'themselves', 'lycians', 'immortals', 'myrmidons', 'gods']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss: 0.3799111843109131\n",
    "cross entropy, 4 epochs\n",
    "he ['she', 'permitted', 'they', 'maris', 'accustomed', 'swooping', 'curiously', 'abydos', 'kneading', 'eilesium']\n",
    "she ['minerva', 'they', 'juno', 'he', 'her', 'venus', 'empty', 'iris', 'husband', 'permitted']\n",
    "ulysses ['penelope', 'telemachus', 'alcinous', 'eurymachus', 'eumaeus', 'antinous', 'mercury', 'leto', 'nausicaa', 'euryclea']\n",
    "penelope ['eumaeus', 'euryclea', 'telemachus', 'antinous', 'dear', 'nausicaa', 'alcinous', 'nurse', 'eurymachus', 'ulysses']\n",
    "achaeans ['trojans', 'danaans', 'others', 'argives', 'gods', 'sea', 'ships', 'suitors', 'forever', 'yourselves']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'ground', 'others', 'gods', 'sea', 'suitors', 'ships', 'earth']\n",
    "\n",
    "Loss: 0.42960622906684875\n",
    "he ['she', 'they', 'cooling', 'we', 'wain', 'i', 'pretty', 'morn', 'reeds', 'scream']\n",
    "she ['he', 'minerva', 'they', 'achilles', 'calypso', 'iris', 'euryclea', 'thersites', 'mercury', 'maids']\n",
    "ulysses ['telemachus', 'antinous', 'agamemnon', 'achilles', 'alcinous', 'penelope', 'menelaus', 'nestor', 'euryclea', 'eumaeus']\n",
    "penelope ['telemachus', 'eumaeus', 'nurse', 'euryclea', 'dear', 'antinous', 'juno', 'alcinous', 'menelaus', 'sir']\n",
    "achaeans ['argives', 'danaans', 'trojans', 'others', 'suitors', 'sun', 'gods', 'immortals', 'phaeacians', 'lycians']\n",
    "trojans ['achaeans', 'argives', 'danaans', 'plain', 'themselves', 'break', 'grow', 'suitors', 'ships', 'mountains']\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
