{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple implementation of skipgrams with negative sampling\n",
    "Author: Pierre Nugues\n",
    "\n",
    "Adapted from _Skipgrams with negative sampling from Distributed Representations of Words and Phrases and their Compositionality_ by Mikolov et al. 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, Average, GlobalAveragePooling1D, Dot, Input, Reshape, Activation\n",
    "import regex as re\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "from random import shuffle, randint\n",
    "from collections import Counter\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding size, context size, and negative counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "w_size = 2\n",
    "c_size = w_size * 2 + 1\n",
    "K_NEG = 5\n",
    "t = 1e-3\n",
    "power = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a dataset and execute locally or on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'homer'  # 'homer' dickens' 'selma' 'big'\n",
    "colab = False # On my machine or on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    BASE_PATH = '/content/drive/My Drive/Colab Notebooks/'\n",
    "else:\n",
    "    BASE_PATH = '../../../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_corpus(path):\n",
    "    files = get_files(path, 'txt')\n",
    "    files = [path + file for file in files]\n",
    "    print(files)\n",
    "    text = ''\n",
    "    for file in files:\n",
    "        text += open(file).read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'homer':\n",
    "    #text = 'Sing, O goddess, the anger of Achilles son of Peleus'.lower()\n",
    "    text1 = open(BASE_PATH + 'corpus/iliad.mb.txt', encoding='utf-8').read().lower()\n",
    "    text2 = open(BASE_PATH + 'corpus/odyssey.mb.txt', encoding='utf-8').read().lower()\n",
    "    text = text1 + text2\n",
    "    test_words = ['he', 'she', 'ulysses', 'penelope', 'achaeans', 'trojans']\n",
    "if dataset == 'dickens':\n",
    "    path = BASE_PATH + 'corpus/Dickens/'\n",
    "    text = load_corpus(path)\n",
    "    test_words = ['he', 'she', 'paris', 'london', 'table', 'rare', 'monday', 'sunday', 'man', 'woman', 'king', 'queen', 'boy',\n",
    "                  'girl']\n",
    "elif dataset == 'selma':\n",
    "    path = BASE_PATH + 'corpus/Selma/'\n",
    "    text = load_corpus(path)\n",
    "    test_words = ['han', 'hon', 'att', 'bord', 'bordet', 'måndag', 'söndag', 'man', 'kvinna', 'kung', 'drottning',\n",
    "                  'pojke', 'flicka']\n",
    "elif dataset == 'big':\n",
    "    path = BASE_PATH + 'corpus/Dickens/'\n",
    "    text = load_corpus(path)\n",
    "    path = BASE_PATH + 'corpus/Norvig/'\n",
    "    text += load_corpus(path)\n",
    "    test_words = ['he', 'she', 'paris', 'london', 'table', 'rare', 'monday', 'sunday', 'man', 'woman', 'king', 'queen', 'boy',\n",
    "                  'girl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set all the text in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'i', 'the', 'quarrel', 'between']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "word_seq = re.findall('\\p{L}+', text)\n",
    "word_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can downsample the frequent words. We first count the words. We will have to count them again after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts['penelope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_probs = dict(counts)\n",
    "word_cnt = sum(discard_probs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in discard_probs:\n",
    "    discard_probs[key] = max(0, 1 - math.sqrt(t/(discard_probs[key]/word_cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8690560952429589"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discard_probs['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7602888379452187"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discard_probs['he']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_word_seq = []\n",
    "for word in word_seq:\n",
    "    if discard_probs[word] < np.random.random():\n",
    "        subsampled_word_seq += [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq = subsampled_word_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172113"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnt = sum(counts.values())\n",
    "word_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abantes',\n",
       " 'abarbarea',\n",
       " 'abas',\n",
       " 'abate',\n",
       " 'abated',\n",
       " 'abetting',\n",
       " 'abhorred',\n",
       " 'abians',\n",
       " 'abide']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = sorted(list(counts.keys()))\n",
    "unique_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9725"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(unique_words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: i for (i, word) in enumerate(unique_words)}\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "#word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We map the words to their indices and we get the sequence of word indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1037, 6666, 897, 187, 67]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widx_seq = list(map(word2idx.get, word_seq))\n",
    "widx_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a power tranform to a list of counts and we return power transformed probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform(counts, power):\n",
    "    trfmd_probs = dict()\n",
    "    for word in counts:\n",
    "        trfmd_probs[word] = math.pow(counts[word], power)\n",
    "    sum_probs = sum(trfmd_probs.values())\n",
    "    for word in trfmd_probs:\n",
    "        trfmd_probs[word] /= sum_probs\n",
    "    return trfmd_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfmd_probs = power_transform(counts, power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the index and proability lists for the random choice function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfmd_probs_idx = {word2idx[k]: v for k, v in trfmd_probs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_idx, probs = zip(*trfmd_probs_idx.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4115,\n",
       " 8488,\n",
       " 2652,\n",
       " 2470,\n",
       " 7191,\n",
       " 8772,\n",
       " 6421,\n",
       " 6755,\n",
       " 3305,\n",
       " 9419,\n",
       " 5719,\n",
       " 6634,\n",
       " 2627,\n",
       " 5047,\n",
       " 9340,\n",
       " 2999,\n",
       " 8957,\n",
       " 9104,\n",
       " 3883,\n",
       " 3970]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(draw_idx, weights=probs, k=K_NEG * 2 * w_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the words, we form positive and negative pairs. We extract the context words of a word from its neighbors in the word sequence to form the positive pairs and at random to form the negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172113it [04:02, 709.64it/s] \n"
     ]
    }
   ],
   "source": [
    "positive_pairs = []\n",
    "negative_pairs = []\n",
    "for idx, widx in tqdm(enumerate(widx_seq)):\n",
    "    # We create the start and end indices as in range(start, end)\n",
    "    start_idx = max(0, idx - w_size)\n",
    "    end_idx = min(idx + w_size + 1, len(widx_seq))\n",
    "    # We create pairs from the left context: start_idx -> idx\n",
    "    for c_idx in range(start_idx, idx):\n",
    "        positive_pairs += [(widx_seq[idx], widx_seq[c_idx])]\n",
    "        negative_pairs += [(widx_seq[idx], neg) for neg in random.choices(draw_idx, weights=probs, k=K_NEG)]\n",
    "    # We create pairs from the right context idx + 1 -> end_idx\n",
    "    for c_idx in range(idx + 1, end_idx):\n",
    "        positive_pairs += [(widx_seq[idx], widx_seq[c_idx])]\n",
    "        negative_pairs += [(widx_seq[idx], neg) for neg in random.choices(draw_idx, weights=probs, k=K_NEG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = positive_pairs + negative_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build two inputs: The left input is the input word and the right one is a context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pairs)\n",
    "X_i = X[:, 0]\n",
    "X_c = X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_word = Input(shape=(1,))\n",
    "i_embedding = Embedding(vocab_size, embedding_dim, input_length=1)(i_word)\n",
    "\n",
    "c_word = Input(shape=(1,))\n",
    "c_embedding = Embedding(vocab_size, embedding_dim, input_length=1)(c_word)\n",
    "\n",
    "dot_prod = Dot(axes=-1)([i_embedding, c_embedding])\n",
    "output = Dense(1, activation='sigmoid')(dot_prod)\n",
    "model = Model([i_word, c_word], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_neg(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    log_y_pred_1 = tf.math.log(y_pred)\n",
    "    log_y_pred_0 = tf.math.log(1.0 - y_pred)\n",
    "    loss = tf.math.add(tf.math.multiply(y_true, log_y_pred_1),\n",
    "                       tf.math.multiply(1.0 - y_true, log_y_pred_0))\n",
    "    loss = -tf.reduce_sum(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss_neg, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_i, X_c], y, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim_vecs(vector, U, nbr_words=10):\n",
    "    # Here cosine distance and not cosine\n",
    "    # distance between equal vectors: 0. max distance: 2\n",
    "    dist = [cosine(vector, U[i, :]) if np.any(U[i, :]) else 2\n",
    "            for i in range(U.shape[0])]\n",
    "    sorted_vectors = sorted(range(len(dist)), key=lambda k: dist[k])\n",
    "    return sorted_vectors[1:nbr_words + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.get_weights()[0]\n",
    "most_sim_words = {}\n",
    "for w in test_words:\n",
    "    most_sim_words[w] = most_sim_vecs(vectors[word2idx[w]], vectors)\n",
    "    most_sim_words[w] = list(map(idx2word.get, most_sim_words[w]))\n",
    "    print(w, most_sim_words[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "base 2 epochs\n",
    "he ['i', 'you', 'a', 'his', 'him', 'as', 'with', 'that', 'for', 'in']\n",
    "she ['her', 'who', 'no', 'or', 'their', 'this', 'be', 'your', 'one', 'said']\n",
    "ulysses ['s', 'out', 'us', 'minerva', 'there', 'achilles', 'now', 'went', 'has', 'came']\n",
    "penelope ['each', 'hands', 'gone', 'battle', 'looking', 'drink', 'telemachus', 'priam', 'same', 'fight']\n",
    "achaeans ['ships', 'trojans', 'city', 'gods', 'sea', 'suitors', 'town', 'darkness', 'meanwhile', 'house']\n",
    "trojans ['sea', 'achaeans', 'ships', 'suitors', 'city', 'getting', 'goddess', 'gods', 'house', 'before']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "power transform 2 epochs\n",
    "he ['i', 'you', 'as', 'him', 'not', 'that', 'it', 'they', 'was', 'had']\n",
    "she ['your', 'us', 'take', 'back', 'some', 's', 'may', 'go', 'let', 'which']\n",
    "ulysses ['this', 'what', 'could', 'spoke', 'can', 'make', 'answered', 'made', 'hector', 'telemachus']\n",
    "penelope ['find', 'let', 'than', 'think', 'noble', 'bring', 'back', 'himself', 'olympus', 'great']\n",
    "achaeans ['among', 'gods', 'out', 'same', 'ships', 'city', 'where', 'house', 'gates', 'suitors']\n",
    "trojans ['achaeans', 'ranks', 'rest', 'among', 'ground', 'others', 'suitors', 'island', 'dust', 'gates']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "power transform 2 epochs, neg loss\n",
    "he ['you', 'i', 'it', 'cover', 'will', 'have', 'that', 'him', 'was', 'heal']\n",
    "she ['ulysses', 'troy', 'did', 'long', 'jove', 'battle', 'very', 'put', 'her', 'think']\n",
    "ulysses ['jove', 'be', 'battle', 'go', 'did', 'thus', 'after', 'think', 'friends', 'never']\n",
    "penelope ['round', 'house', 'keep', 'host', 'strength', 'achaeans', 'presently', 'whom', 'servants', 'both']\n",
    "achaeans ['into', 'house', 'other', 'gods', 'fire', 'both', 'city', 'host', 'king', 'back']\n",
    "trojans ['into', 'other', 'about', 'themselves', 'away', 'sea', 'through', 'among', 'achaeans', 'back']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "power transform 8 epochs, neg loss\n",
    "he ['you', 'i', 'it', 'cover', 'will', 'have', 'that', 'him', 'was', 'heal']\n",
    "she ['ulysses', 'troy', 'did', 'long', 'jove', 'battle', 'very', 'put', 'her', 'think']\n",
    "ulysses ['jove', 'be', 'battle', 'go', 'did', 'thus', 'after', 'think', 'friends', 'never']\n",
    "penelope ['round', 'house', 'keep', 'host', 'strength', 'achaeans', 'presently', 'whom', 'servants', 'both']\n",
    "achaeans ['into', 'house', 'other', 'gods', 'fire', 'both', 'city', 'host', 'king', 'back']\n",
    "trojans ['into', 'other', 'about', 'themselves', 'away', 'sea', 'through', 'among', 'achaeans', 'back']\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
